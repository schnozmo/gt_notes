<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Computational Learning Theory, Supervised Learning"/><meta name="created" content="2017-01-27 21:01:49 +0000"/><meta name="source" content="mobile.ipad"/><meta name="updated" content="2017-02-02 05:28:16 +0000"/><title>SL7 - Computational Learning Theory</title></head><body><div/><div/><div><span><a href="http://upload.wikimedia.org/wikipedia/en/f/fe/Mondrian_Composition_II_in_Red%2C_Blue%2C_and_Yellow.jpg">Mondrain Composition</a><br/><a href="http://upload.wikimedia.org/wikipedia/commons/2/20/Coloured_Voronoi_2D.svg">Colored Vornoi Diagram</a></span><br/></div><div><br/></div><div>Computational Learning Theory questions</div><div>- Defining learning problems</div><div>- Showing specific algorithms work</div><div>- Show these problems are fundamentally hard</div><div><br/></div><div>What resources matter?</div><div>- time</div><div>- space</div><div>- data/examples/samples</div><div><br/></div><div>Properties of Inductive Learners</div><div>- probability of successful training —&gt; 1 - delta</div><div>- number of examples to train on  —&gt; m</div><div>- complexity of hypothesis class, —&gt; complexity of H</div><div>- accuracy to which target concept is approximated —&gt; epsilon</div><div>- manner in which trng examples presented —&gt; batch (bunch of training examples, then fit) or “online” (fit on one at a time)</div><div>- manner in which trng examples selected —&gt; teacher gives x, c(x) pairs OR learner asks teacher for c(x) given x OR fixed distribution</div><div>    - teaching via 20 questions - H: set of possible people, X: set of questions. If <u>teacher chooses</u> X, the X it gives the leads directly to the answer. If <u>learner chooses</u> X, it will want to divide the hypothesis space in half or log_2(H).<br/></div><div><span><br/></span></div><div><span><br/></span></div><div><b><span>Teaching with constrained queries</span><br/></b></div><div>X: x1, x2, …, xk</div><div>H: conjuctions of literals or negation (e.g., h: x1 and x3 and !x5)</div><div><br/></div><div><table style="width: 48.10996563573883%; border-collapse: collapse; table-layout: fixed;"><tbody><tr><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X1</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X2</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X3</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X4</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X5</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">h</b></div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">0</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr></tbody></table><br/></div><div>what boolean conjunction of x’s predict h:</div><div><span>- show what’s irrelevant, then show what’s relevant</span><br/></div><div>- in the first 2 rows x1 and x3 change, without a corresponding change in h, so those are <b>irrelevant</b></div><div>- for the first 2 rows, h = 1 only when <b>x4 and !x2 and !x5</b></div><div>- does this work for remaining rows, where h is 0? yes!</div><div><br/></div><div>Learners with constrained queries have to propose X values to get h value. If we’re trying to determine h where 5 variables are all included, guessing Xs that yield h(x) = 1 is really hard, because there will be exactly one set of Xs that produce h(x) = 1. Since there are 5 variables with 2 choices (positive or negation), that’s 2 ** 5 possible guesses (same as H) until I find the right answer</div><div><br/></div><div>Learners with mistake bounds - learn from mistakes</div><div>- Loop</div><div><span>    </span>- input arrives</div><div><span>    </span>- learner guesses answer</div><div><span>    </span>- wrong answer “charged"</div><div><br/></div><div>1 - assume it’s possible each variable could be positive <u>and</u> negated</div><div>2 - given input, compute output</div><div>3 - if wrong, set all positive variables that were 0 to absent, negative variables that were 1 to absent, goto 2</div><div><br/></div><div/><div><table style="width: 95.41809851088202%; border-collapse: collapse; table-layout: fixed;"><tbody><tr><td style="width: 13.968253968253968%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><font style="font-weight: bold;">possibilities</font><br/></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X1</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X2</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X3</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X4</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X5</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">h</b></div></td><td style="width: 34.69387755102041%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b>notes</b></div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>x1-x5 = {+,!}</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">0</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.47732696897374705%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.408997955010225%;"><div>x1!, x2+, x3!, x4!, x5+ can’t be part of answer</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>x1,x3,x4 = {+}</div><div>x2,x5 = {!}</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.47732696897374705%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.408997955010225%;"><div>x5! not part of the answer, move x5 to absent - further examples of x5 not needed</div></td></tr></tbody></table></div><div><br/></div><div>never make more than k+1 mistakes</div><div><br/></div><div>computation complexity - how much computation effort is needed for a learner to converge?</div><div>sample complexity (batch) - how many trng examples reqd for a learner to create a successful/best hypothesis</div><div>mistake bounds (online) - how many misclassifications can a learner make over an infinite run</div><div/><div> </div><div>Version spaces</div><div><br/></div><div>true hypothesis/concept - c member of H</div><div>candidate hypothesis -      h member of H</div><div>training set -                      S subset of X</div><div>consistent learner produces c(x) = h(x) for all x member of S</div><div>version space - VS(S) = {h such that h member of H consistent wrt S} - set of h consistent for S</div><div>training error - fraction of trng examples misclassified by h</div><div>true error - fraction of examples that <u>would</u> be misclassified on sample drawn from D</div><div><span>     error_D(h) = Pr_x~D[ c(x) != h(x) ]</span><br/></div><div><br/></div><div>C - concept class</div><div>L - learner</div><div>H - hypothesis space</div><div>n - size of H</div><div>D - distribution of inputs</div><div>epsilon - error goal, max error</div><div>delta - certainty goal (works with Pr[ 1 - delta ] )</div><div><br/></div><div>VS(S) epsilon-exhausted iff for all h member of VS(S), error_D(h) &lt;= epsilon, VS is consistent hypothesis</div><div><br/></div><div>Haussler’s Theorem - bound true error based on sample set</div><div>- let error_D(h1,…,hk member of H) &gt; epsilon    ( high true error )</div><div>- how much data do we need to remove these hypotheses</div><div>- Pr_x~D[ h_i(x) = c(x) ] &lt;= 1-epsilon     “low” probability of match</div><div>- Pr [h_i(x) = c(x) on m examples] &lt;= (1 - epsilon) ** m</div><div>- Pr [ at least one of h1,…,hk remains consistent with c on m examples ] &lt;= k * (1 - epsilon) ** m &lt;= len(H) * (1 - epsilon) ** m</div><div>- we know (1 - epsilon) ** m &lt;= e**(-epsilon * m), so upper bound is len(H) * e**(-epsilon * m)</div><div>- rewritten for m, m &gt;= ( 1/epsilon ) * ( ln|H| + ln( 1/delta )</div><div><br/></div><div><br/></div><div>how many samples needed to PAC-learn this H:</div><div>    |H| = 10, ln|H| = 2.3,    1/delta = 5, ln|H| = 1.6, ln|H| + ln(1/delta) = ~4, 4 * 1/.1 = <b>40</b></div><div><br/></div><div>minimum sample size = 40</div><div><br/></div><div>Summary</div><div>- teacher/learner interaction and speed of learning</div><div><span>    - learner picks questions</span><br/></div><div><span><span>    - teacher picks questions</span><br/></span></div><div><span><span><span>    - nature presents questions</span><br/></span></span></div><div>- what is learnable</div><div>- sample complexity</div><div><br/></div><div><br/></div><div><b><font style="font-size: 18px;">Mitchell Book chapter 7</font></b></div><div><br/></div><div>What general laws govern learning?</div><div><br/></div><div>It is possible to set quantitative bounds on:</div><div><ul><li><span>Sample complexity - HM trng samples reqd to learn tgt fxn</span><br/></li><li>Mistake bound - HM mistakes will learner make before succeeding</li><li>Computational complexity - HM comp effort reqd for a learner to converge to a successful hypo</li></ul><div><br/></div></div><div>based on </div><div><ul><li>size/complexity of hypo space</li><li>accuracy to which tgt hypo must be approximated</li><li>P(learner will output successful hypo)</li><li>how trng examples presented to learner</li></ul><div><br/></div><div><br/></div><div><span style=""><b>Probably Approximately Correct learning setting</b></span></div><div><br/></div><div>interested in characterizing learner L using various hypo spaces H when learning individual tgt concepts drawn from classes C. Because we want L to be general enough to learn any tgt concept from C regardless of the distribution of trng examples, we will bemost interestedin <u>worst-case</u> analyses over all tgt concepts from C and all poss instance distributions D</div><div><br/></div><div>True error of hypotheseis h - probability that h is misclassify an instance drawn according to D</div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/Evernote%20Snapshot%2020170127%20162754.jpg" height="377" width="1990"/><br/></div><div><br/></div><div>This error is defined from the set of all instances, not just training and test sets. Therefore, true error depends on the uknown contents of the entire distribution D. If D is uniformly distributed, the true error will be equal to the space where c and h disagree. However, TE will be higher if D emphasizes regions where c and h disagree.</div><div><br/></div><div>This leads to the question for each learner of how probable is it that the training error for h gives a misleading estimate of true error</div><div><br/></div><div>PAC Learnability</div><div>- goal to char classes of tgt concepts that can reliably learned from a reasonable number of randomly drawn trng examples and a reasonable amt of computation</div><div><br/></div><div>we can't expect to put bounds on error_D(h) = 0 (can' provide as trng all instances in X; trng examples could be misleading), so we shoot for error_D(h) = epsilon and don' require that it succeed for every seq of trng examples (<u>we want to PROBABLY learn an APPROXIMATELTY CORRECT hyporthesis</u>)</div></div><div><br/></div><div>epsilon = max error for h</div><div>delta = probability of failure to find h for subset of D over X, such that error &lt; epsilon <br/></div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/Evernote%20Snapshot%2020170127%20171616.jpg" height="472" width="2136"/><br/></div><div><br/></div><div>L must have a high probability, 1-delta, output an h with error &lt; epsilon. L must also do so efficiently with respect to 1/epsilon and 1/theta (output parameters) and n and size(c) (input parameters).</div><div><br/></div><div>this does assume that H <i>contains</i> an h with a small enough error for <i>every</i> concept in C. Some problems will not have complete C (faces in image recognition). </div><div><br/></div><div>Sample Complexity</div><div><ul><li>growth in number of reqd trng examples with problem size</li><li>consistent learner - a learner that outputs hypotheses that perfectly fit the trng data whenever possible</li><ul><li>we can derive a bound on the number of trng examples reqd by any consistent learner, independent of the specific algorithm it uses. </li><li>Version Space - set of all h subset of H that correctly classify trng examples D</li><li>every consistent learner outputs a successful hypothesis belonging to the Version Space</li><li>therefore, bounding the number of examples needed by a consistent learner means we only need to bound the number of examples needed to assure that the Version Space contains no unacceptable hypotheses</li><li>Haussler (1988) - Version Space VS_H,D is<b> epsilon-exhausted</b> wrt c and D if every h in VS_H,D has error &lt; epsilon</li><li>probability that VS_H,D is <u style="font-style: italic;">not</u> epsilon-exhausted is &lt;= len(H) * e**-(epsilon * m)</li><li>this bounds the probability that m trng examples will fail to eliminate all bad h (h with true error &gt; epsilon) for any consistent learner using H</li><li><b>solving for m</b>,  <img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/F092837A-2F0C-420A-830D-2BB19A76A81C.gif" height="19" width="211"/> </li><li>m grows log wrt H and delta, lin wrt to epsilon</li></ul><li>agnostic learner - learner makes no assumption that tgt concept is representable by H and simply finds the minimum trng error</li><ul><li>h_best - hypothesis from H with the lowest trng error_D(h)</li><li>how many trng examples needed to ensure that true error = trng error + epsilon?</li><li>Hoeffding bounds - Pr[error_D(h) &gt; error_D(h) + epsilon] &lt;= e ** -(2m*epsilon**2) - bound on Pr arbitrarily chosen h has misleading trng error</li><li>delta = Pr_exists_h_in_H[ error_D(h) &gt; error_D(h) + epsilon] &lt;= len(H)*e ** -(2m*epsilon**2)</li><li><b>solving for m</b>,  <img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/56B61316-3F5B-4AFB-B37A-AC61272B1262.gif" height="21" width="227"/><br/></li></ul></ul></div><div><br/></div><hr/>Conflicting modification on January 29, 2017 at 9:08:32 PM:<hr/><div/><div/><div><span><a href="http://upload.wikimedia.org/wikipedia/en/f/fe/Mondrian_Composition_II_in_Red%2C_Blue%2C_and_Yellow.jpg">Mondrain Composition</a><br/><a href="http://upload.wikimedia.org/wikipedia/commons/2/20/Coloured_Voronoi_2D.svg">Colored Vornoi Diagram</a></span><br/></div><div><br/></div><div>Computational Learning Theory questions</div><div>- Defining learning problems</div><div>- Showing specific algorithms work</div><div>- Show these problems are fundamentally hard</div><div><br/></div><div>What resources matter?</div><div>- time</div><div>- space</div><div>- data/examples/samples</div><div><br/></div><div>Properties of Inductive Learners</div><div>- probability of successful training —&gt; 1 - delta</div><div>- number of examples to train on  —&gt; m</div><div>- complexity of hypothesis class, —&gt; complexity of H</div><div>- accuracy to which target concept is approximated —&gt; epsilon</div><div>- manner in which trng examples presented —&gt; batch (bunch of training examples, then fit) or “online” (fit on one at a time)</div><div>- manner in which trng examples selected —&gt; teacher gives x, c(x) pairs OR learner asks teacher for c(x) given x OR fixed distribution</div><div>    - teaching via 20 questions - H: set of possible people, X: set of questions. If <u>teacher chooses</u> X, the X it gives the leads directly to the answer. If <u>learner chooses</u> X, it will want to divide the hypothesis space in half or log_2(H).<br/></div><div><span><br/></span></div><div><span><br/></span></div><div><b><span>Teaching with constrained queries</span><br/></b></div><div>X: x1, x2, …, xk</div><div>H: conjuctions of literals or negation (e.g., h: x1 and x3 and !x5)</div><div><br/></div><div><table style="width: 48.10996563573883%; border-collapse: collapse; table-layout: fixed;"><tbody><tr><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X1</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X2</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X3</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X4</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X5</b></div></td><td style="width: 16.666666666666664%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">h</b></div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">0</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td></tr></tbody></table><br/></div><div>what boolean conjunction of x’s predict h:</div><div><span>- show what’s irrelevant, then show what’s relevant</span><br/></div><div>- in the first 2 rows x1 and x3 change, without a corresponding change in h, so those are <b>irrelevant</b></div><div>- for the first 2 rows, h = 1 only when <b>x4 and !x2 and !x5</b></div><div>- does this work for remaining rows, where h is 0? yes!</div><div><br/></div><div>Learners with constrained queries have to propose X values to get h value. If we’re trying to determine h where 5 variables are all included, guessing Xs that yield h(x) = 1 is really hard, because there will be exactly one set of Xs that produce h(x) = 1. Since there are 5 variables with 2 choices (positive or negation), that’s 2 ** 5 possible guesses (same as H) until I find the right answer</div><div><br/></div><div>Learners with mistake bounds - learn from mistakes</div><div>- Loop</div><div><span>    </span>- input arrives</div><div><span>    </span>- learner guesses answer</div><div><span>    </span>- wrong answer “charged"</div><div><br/></div><div>1 - assume it’s possible each variable could be positive <u>and</u> negated</div><div>2 - given input, compute output</div><div>3 - if wrong, set all positive variables that were 0 to absent, negative variables that were 1 to absent, goto 2</div><div><br/></div><div/><div><table style="width: 95.41809851088202%; border-collapse: collapse; table-layout: fixed;"><tbody><tr><td style="width: 13.968253968253968%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><font style="font-weight: bold;">possibilities</font><br/></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X1</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X2</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X3</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X4</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">X5</b></div></td><td style="width: 8.403361344537815%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b style="">h</b></div></td><td style="width: 34.69387755102041%; border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div><b>notes</b></div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>x1-x5 = {+,!}</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">0</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;">1</td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.47732696897374705%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.408997955010225%;"><div>x1!, x2+, x3!, x4!, x5+ can’t be part of answer</div></td></tr><tr><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>x1,x3,x4 = {+}</div><div>x2,x5 = {!}</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.4472271914132379%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>0</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.47732696897374705%;"><div>1</div></td><td style="border: 1px solid rgb(219, 219, 219); padding: 10px; margin: 0px; width: 0.408997955010225%;"><div>x5! not part of the answer, move x5 to absent - further examples of x5 not needed</div></td></tr></tbody></table></div><div><br/></div><div>never make more than k+1 mistakes</div><div><br/></div><div>computation complexity - how much computation effort is needed for a learner to converge?</div><div>sample complexity (batch) - how many trng examples reqd for a learner to create a successful/best hypothesis</div><div>mistake bounds (online) - how many misclassifications can a learner make over an infinite run</div><div/><div> </div><div>Version spaces</div><div><br/></div><div>true hypothesis/concept - c member of H</div><div>candidate hypothesis -      h member of H</div><div>training set -                      S subset of X</div><div>consistent learner produces c(x) = h(x) for all x member of S</div><div>version space - VS(S) = {h such that h member of H consistent wrt S} - set of h consistent for S</div><div>training error - fraction of trng examples misclassified by h</div><div>true error - fraction of examples that <u>would</u> be misclassified on sample drawn from D</div><div><span>     error_D(h) = Pr_x~D[ c(x) != h(x) ]</span><br/></div><div><br/></div><div>C - concept class</div><div>L - learner</div><div>H - hypothesis space</div><div>n - size of H</div><div>D - distribution of inputs</div><div>epsilon - error goal, max error</div><div>delta - certainty goal (works with Pr[ 1 - delta ] )</div><div><br/></div><div>VS(S) epsilon-exhausted iff for all h member of VS(S), error_D(h) &lt;= epsilon, VS is consistent hypothesis</div><div><br/></div><div>Haussler’s Theorem - bound true error based on sample set</div><div>- let error_D(h1,…,hk member of H) &gt; epsilon    ( high true error )</div><div>- how much data do we need to remove these hypotheses</div><div>- Pr_x~D[ h_i(x) = c(x) ] &lt;= 1-epsilon     “low” probability of match</div><div>- Pr [h_i(x) = c(x) on m examples] &lt;= (1 - epsilon) ** m</div><div>- Pr [ at least one of h1,…,hk remains consistent with c on m examples ] &lt;= k * (1 - epsilon) ** m &lt;= len(H) * (1 - epsilon) ** m</div><div>- we know (1 - epsilon) ** m &lt;= e**(-epsilon * m), so upper bound is len(H) * e**(-epsilon * m)</div><div>- rewritten for m, m &gt;= ( 1/epsilon ) * ( ln|H| + ln( 1/delta )</div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/8448E5C3-F63F-4740-978D-3A38F16CB688.png" height="107" width="485"/><br/></div><div><br/></div><div>how many samples needed to PAC-learn this H:</div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/AD140760-9A2D-4A54-A70B-5050D9B66943.png" height="314" width="272"/>    |H| = 10, ln|H| = 2.3,    1/delta = 5, ln|H| = 1.6, ln|H| + ln(1/delta) = ~4, 4 * 1/.1 = <b>40</b></div><div><br/></div><div>minimum sample size = 40</div><div><br/></div><div>Summary</div><div>- teacher/learner interaction and speed of learning</div><div><span>    - learner picks questions</span><br/></div><div><span><span>    - teacher picks questions</span><br/></span></div><div><span><span><span>    - nature presents questions</span><br/></span></span></div><div>- what is learnable</div><div>- sample complexity</div><div><br/></div><div><br/></div><div><b><font style="font-size: 18px;">Mitchell Book chapter 7</font></b></div><div><br/></div><div>What general laws govern learning?</div><div><br/></div><div>It is possible to set quantitative bounds on:</div><div><ul><li><span>Sample complexity - HM trng samples reqd to learn tgt fxn</span><br/></li><li>Mistake bound - HM mistakes will learner make before succeeding</li><li>Computational complexity - HM comp effort reqd for a learner to converge to a successful hypo</li></ul><div><br/></div></div><div>based on </div><div><ul><li>size/complexity of hypo space</li><li>accuracy to which tgt hypo must be approximated</li><li>P(learner will output successful hypo)</li><li>how trng examples presented to learner</li></ul><div><br/></div><div><br/></div><div><span style=""><b>Probably Approximately Correct learning setting</b></span></div><div><br/></div><div>interested in characterizing learner L using various hypo spaces H when learning individual tgt concepts drawn from classes C. Because we want L to be general enough to learn any tgt concept from C regardless of the distribution of trng examples, we will bemost interestedin <u>worst-case</u> analyses over all tgt concepts from C and all poss instance distributions D</div><div><br/></div><div>True error of hypotheseis h - probability that h is misclassify an instance drawn according to D</div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/Evernote%20Snapshot%2020170127%20162754.jpg" height="377" width="1990"/><br/></div><div><br/></div><div>This error is defined from the set of all instances, not just training and test sets. Therefore, true error depends on the uknown contents of the entire distribution D. If D is uniformly distributed, the true error will be equal to the space where c and h disagree. However, TE will be higher if D emphasizes regions where c and h disagree.</div><div><br/></div><div>This leads to the question for each learner of how probable is it that the training error for h gives a misleading estimate of true error</div><div><br/></div><div>PAC Learnability</div><div>- goal to char classes of tgt concepts that can reliably learned from a reasonable number of randomly drawn trng examples and a reasonable amt of computation</div><div><br/></div><div>we can't expect to put bounds on error_D(h) = 0 (can' provide as trng all instances in X; trng examples could be misleading), so we shoot for error_D(h) = epsilon and don' require that it succeed for every seq of trng examples (<u>we want to PROBABLY learn an APPROXIMATELTY CORRECT hyporthesis</u>)</div></div><div><br/></div><div>epsilon = max error for h</div><div>delta = probability of failure to find h for subset of D over X, such that error &lt; epsilon <br/></div><div><img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/Evernote%20Snapshot%2020170127%20171616.jpg" height="472" width="2136"/><br/></div><div><br/></div><div>L must have a high probability, 1-delta, output an h with error &lt; epsilon. L must also do so efficiently with respect to 1/epsilon and 1/theta (output parameters) and n and size(c) (input parameters).</div><div><br/></div><div>this does assume that H <i>contains</i> an h with a small enough error for <i>every</i> concept in C. Some problems will not have complete C (faces in image recognition). </div><div><br/></div><div>Sample Complexity</div><div><ul><li>growth in number of reqd trng examples with problem size</li><li>consistent learner - a learner that outputs hypotheses that perfectly fit the trng data whenever possible</li><ul><li>we can derive a bound on the number of trng examples reqd by any consistent learner, independent of the specific algorithm it uses. </li><li>Version Space - set of all h subset of H that correctly classify trng examples D</li><li>every consistent learner outputs a successful hypothesis belonging to the Version Space</li><li>therefore, bounding the number of examples needed by a consistent learner means we only need to bound the number of examples needed to assure that the Version Space contains no unacceptable hypotheses</li><li>Haussler (1988) - Version Space VS_H,D is<b> epsilon-exhausted</b> wrt c and D if every h in VS_H,D has error &lt; epsilon wrt c and D.</li><li>probability that VS_H,D is <u style="font-style: italic;">not</u> epsilon-exhausted is &lt;= len(H) * e**-(epsilon * m)</li><li>this bounds the probability that m trng examples will fail to eliminate all bad h (h with true error &gt; epsilon) for any consistent learner using H</li><li><b>solving for m</b>,  <img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/F092837A-2F0C-420A-830D-2BB19A76A81C.gif" height="19" width="211"/> </li><li>m grows log wrt H and delta, lin wrt to epsilon</li></ul><li>agnostic learner - learner makes no assumption that tgt concept is representable by H and simply finds the minimum trng error</li><ul><li>h_best - hypothesis from H with the lowest trng error_D(h)</li><li>how many trng examples needed to ensure that true error = trng error + epsilon?</li><li>Hoeffding bounds - Pr[error_D(h) &gt; error_D(h) + epsilon] &lt;= e ** -(2m*epsilon**2) - bound on Pr arbitrarily chosen h has misleading trng error</li><li>delta = Pr_exists_h_in_H[ error_D(h) &gt; error_D(h) + epsilon] &lt;= len(H)*e ** -(2m*epsilon**2)</li><li><b>solving for m</b>,  <img src="SL7%20-%20Computational%20Learning%20Theory.html.resources/56B61316-3F5B-4AFB-B37A-AC61272B1262.gif" height="21" width="227"/><br/></li></ul></ul></div><div><br/></div></body></html>
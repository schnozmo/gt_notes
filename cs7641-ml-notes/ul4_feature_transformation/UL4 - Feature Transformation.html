<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Feature Transformation, Unsupervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-03-18 21:32:10 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-03-28 16:54:30 +0000"/><title>UL4 - Feature Transformation</title></head><body><div>The problem of pre-processing a set of features to create a new (smaller? more compact?) feature set while retaining as much (relevant? useful?) information as possible.</div><div><br/></div><div>x is the set of instances, n and m are a number of features</div><div><br/></div><div>x ~ F^n —&gt; F^m, where m &lt; n (almost always) and the transformation is P^T_x (linear, usually)</div><div><br/></div><div>feature selection is x_1, x_2, x_3, x_4 -&gt; x_2, x_4</div><div>feature transformation is x_1, x_2, x_3, x_4 —&gt; 2x_1, x_3</div><div><br/></div><div>staying with linear combinations. Domain knowledge is eliminating unneeded features.</div><div><br/></div><div>Neural networks, through layers of perceptrons, are already doing a kind of feature transformation</div><div><br/></div><div>ad hoc Information Retrieval (i.e., google search)</div><div><br/></div><div>Features</div><div>- words, phrases - documents have a ton of words (curse of dimensionality)</div><div><span>    - some words have different meanings - polysemy</span></div><div><span><span>       </span>- apple (fruit or tech giant)</span></div><div><span><span>       </span>- yields false positives, e.g., searched for apple (fruit), got apple (tech giant)</span><br/></div><div><span><span>    - many words have the same meaning - synonomy</span></span></div><div><span><span><span>      </span>- car and automobile</span></span></div><div><span><span><span>      </span>- yields false negatives, e.g, get documents about cars, but miss documents that don’t contain automobiles </span></span><span> </span></div><div><span><span><br/></span></span></div><div>Feature transformation allows us to combine words like car, automobile, motor, vehicle, etc, into a single feature</div><div><br/></div><div><br/></div><div><b><u>Principal Components Analysis</u></b></div><div><b><u><br/></u></b></div><div><b><u><u><b><a href="http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues</a></b></u><br/></u></b></div><div><b><u><u><br/></u></u></b></div><div>Eigenproblem - An <em>eigenproblem</em> is a computational problem that can be solved by finding the eigenvalues and/or eigenvectors of a matrix. In PCA, we are analyzing the covariance matrix (see the paper for details)</div><div><br/></div><div><span>variance based on single variables, x and y is smaller than x+y, represented by red line</span><br/></div><div><img src="UL4%20-%20Feature%20Transformation.html.resources/BC56B1E9-A946-47A2-9F9F-437C0809E635.png" height="285" width="462"/><br/></div><div><br/></div><div>PCA finds</div><div>(1) directions of maximal variance of data and simplifies as a single definition - <b>principal/first component</b></div><div><div/></div><div>(2) directions that are mutually orthogonal (perpendicular in 2D) - <b>second principal/second component</b></div><div><br/></div><div><b/>Properties of PCA</div><div>- mutually orthogonal means this is a global algorithm. all the new features have a global constraint</div><div>- PCA gives the best reconstruction. point (x1,y1) will be represented as (red1, orange1) on the (1) and (2) axes, but it can be reconstructed. This minimizes L2 error moving from N —&gt; M dimensions (L2 error is a type of squared error) </div><div>- PCA gives new axes, so given n dimensions, n dimensions are returned. What you also get are <b>eigenvalues</b> for each new axis. These eigenvalues are non-negative and monotonically increasing, so they don’t change the data. Dimensions with the smallest eigenvalues have the smallest variance and are candidates for elimination, since they don’t have much of an impact on any result (like a parameter that is almost always the same value). If the eigenvalue for some dimension is zero, it has zero entropy and can be thrown away. </div><div>- well-studied - there exist very fast algorithms to compute principal components</div><div>- This is like filtering feature selection</div><div><br/></div><div>PCA finds things that are uncorrelated.</div><div>If all data is Gaussian, PCA features will also be statistically independent</div><div><br/></div><div><b><u>Independent Components Analysis</u></b></div><div><br/></div><div>Tries to maximize independence, so that new features are mutually independent</div><div>(x_1, x_2, x_i) Linear transformation —&gt; (y_1, y_2, y_3) s.t. I(y_i, y_j) = 0 and I(y, x) = maximal ( I = mutual information ). We want this so that we can reconstruct</div><div><br/></div><div>Blind source separation/cocktail party problem</div><div>- each microphone has a different linear combination of all voices</div><div><img src="UL4%20-%20Feature%20Transformation.html.resources/AB63BF31-0CFC-4A33-9BCE-E227A35017C5.png" height="287" width="384"/><br/></div><div><br/></div><div>Soundwaves get sampled periodically, each sample has a metric. These samples convert to a matrix, where the rows represent microphones and the values represent times.</div><div><br/></div><div>ICA is trying to find projections that are independent. A bunch of independent variables summed together (linear combination), which become Normal (Central Limit Theorem). ICA tries to make mutually independent the new features, but maximize joint mutual information between new and old features. ICA doesn’t care about ordering. </div><div><br/></div><div><br/></div><div/><div><img src="UL4%20-%20Feature%20Transformation.html.resources/0F72A307-09DD-4ECB-9946-150C7FFF94CF.png" height="461" width="910"/><br/></div><div><br/></div><div>ICA and PCA are trying to do the same thing, but with 2 different approaches, assumptions, and goals wrt data</div><div><br/></div><div>Blind source separation - ICA good, PCA bad</div><div><br/></div><div>ICA is directional - whether features are rows and samples are columns or the transpose, ICA finds the same answer.</div><div><br/></div><div>Faces - PCA - focuses on darkness (direction of maximal variance), so first principal component is not useful. 2nd principal component is the “average” face. ICA finds noses, eye, hair, and mouths. ICA finds fundamentals</div><div><br/></div><div>Alternatives to PCA/ICA</div><div><br/></div><div>- RCA (Random Components Analysis/Random Projection) - generates random directions and projects data in those directions. This works really well before doing some classification. Taking random projections still produces some correlations, though usually  there are more output features than in PCA/ICA. Advantage is speed</div><div><br/></div><div>- LDA (Linear Discriminant Analysis) - finds a projection that discriminates based on a label. This is basically supervised learning.</div><div><br/></div><div><a href="UL4%20-%20Feature%20Transformation.html.resources/PCA-Tutorial-Intuition_jp.pdf">PCA-Tutorial-Intuition_jp.pdf</a><br/></div><div><br/></div></body></html>
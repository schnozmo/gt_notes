<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Reinforcement Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-04-09 13:56:50 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-04-12 02:46:39 +0000"/><title>RL2 - Reinforcement Learning</title></head><body><div><p>Andrew Moore's slides on Zero-Sum Games (<a target="_blank" href="http://www.autonlab.org/tutorials/gametheory.html">http://www.autonlab.org/tutorials/gametheory.html</a>)
</p><p>Andrew Moore's slides on Non-Zero-Sum Games (<a target="_blank" href="http://www.autonlab.org/tutorials/nonzerosum.html">http://www.autonlab.org/tutorials/nonzerosum.html</a>)
</p><p><br/></p><p><a target="_blank" rel="noreferrer" href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Richard Sutton and Andrew Barto, Reinforcement Learning: An introduction. MIT Press, 1998.</a></p><div><p/></div><p><a target="_blank" rel="noreferrer" href="https://github.com/pushkar/4641/raw/master/downloads/kaelbling96reinforcement.pdf">Reinforcement Learning: A Survey</a><br/></p><p><a target="_blank" rel="noreferrer" href="http://uhaweb.hartford.edu/compsci/ccli/projects/QLearning.pdf">http://uhaweb.hartford.edu/compsci/ccli/projects/QLearning.pdf</a><br/></p><p><a href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a.pdf">http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a.pdf</a><br/></p><div/></div><div><img src="RL2%20-%20Reinforcement%20Learning.html.resources/187E1AFE-1F12-47B4-95BA-E999E9E0D521.png" height="258" width="789"/><br/></div><div>Planning is kinda the MDP part</div><div><br/></div><div>Animal sees a stimulus (s), takes an action (a), gets a reward (r). </div><div><br/></div><div>reinforcement learning is really reward maximization</div><div><br/></div><div>RL as an API</div><div><br/></div><div>model (T, R) —&gt; <b>Planner</b> —&gt; policy         - computing a policy</div><div>transitions(s,a,r,s’) —&gt; <b>Learner</b> —&gt; policy    - learning/approximating a policy</div><div>transitions(s,a,r,s’) —&gt; <b>Modeler</b> —&gt; model   - SL?</div><div>model —&gt; <b>Simulator</b> —&gt; transitions</div><div><br/></div><div><br/></div><div>one way to glue together is:</div><div><br/></div><div>model —&gt; Simulator —&gt; transitions —&gt; Learner —&gt; policy  ==&gt; RL-based planner</div><div>transitions —&gt; Modeler —&gt; model —&gt; Planner —&gt; policy    ==&gt; model-based RL</div><div><br/></div><div>3 approaches to RL</div><div><br/></div><div>s —&gt; policy —&gt; a (policy search algorithms, direct to goal, but learning is tough, ie temporal credit assignment)</div><div>s —&gt; U —&gt; v (value function based, more direct learning)</div><div>s, a —&gt; T, R —&gt; s’, r’ (predict next state and reward, model-based RL, direct learning (supervised))</div><div><br/></div><div><br/></div><div>T,R —&gt; value-iteration (solve Bellman equations)  —&gt; v —&gt; argmax —&gt; policy</div><div><br/></div><div>Bellman equation - utility/long term value of a state is the reward for being in that state plus discounted reward of the future</div><div>     <img src="RL2%20-%20Reinforcement%20Learning.html.resources/7F3DFEE3-365F-4F7D-A06E-F0CF2F5BAFD3.gif" height="39" width="388"/><br/></div><div>Q function - value for arriving at s, leaving via a, proceeding optimally thereafter </div><div>     <img src="RL2%20-%20Reinforcement%20Learning.html.resources/9993610D-C902-4D43-B586-602C358528A0.gif" height="39" width="446"/><br/></div><div><img src="RL2%20-%20Reinforcement%20Learning.html.resources/8FB5F77E-9A8B-40BB-82F1-A87EF8F05D3D.png" height="479" width="826"/><br/></div><div><br/></div><div><br/></div><div>Estimating Q from transitions - we don’t have R and T. </div><div><br/></div><div>we do have s, a, r, and s’, ie transitions. we are in some state s, an action a was taken where we’re given some reward r, and arrive in new state s'</div><div><br/></div><div>update \hatQ, by moving it a little (learning rate \alpha) towards the reward plus the discounted max reward of the next state.</div><div>                              /        u of current state     \</div><div>                                            / u of next state  \</div><div>       <img src="RL2%20-%20Reinforcement%20Learning.html.resources/B0322B75-4C0F-4DDE-BF05-4AB7A24CBE6D.gif" height="21" width="254"/><br/></div><div><br/></div><div>Notation <img src="RL2%20-%20Reinforcement%20Learning.html.resources/771D4C3E-C472-4FE4-AC79-0591BF85D886.gif" height="20" width="247"/>, \alpha = 1 means no learning, \alpha = 0 means complete overwrite of V</div><div><br/></div><div><img src="RL2%20-%20Reinforcement%20Learning.html.resources/5C07CE9F-6A4B-41AA-A817-45509A7EEFCA.png" height="381" width="824"/><br/></div><div><br/></div><div><img src="RL2%20-%20Reinforcement%20Learning.html.resources/199922E5-AC07-4798-9EB0-1C79C9703BD4.png" height="358" width="849"/><br/></div><div><br/></div><div>Q-learning is a <u>family</u> of algorithms that vary along</div><div>- how to initialize\hatQ</div><div>- how to decay \alpha_t</div><div>- how to choose actions? </div><div><span>    - choose randomly? could learn q, but ignoring it</span><br/></div><div><span><span>    - always choose the same action? would learn anything</span><br/></span></div><div><span><span><span>    - use \hatQ</span><br/></span></span></div><div><br/></div><div>Greedy (local min)</div><div>- foreach state, \hatQ(s, a_0) = “awesome”</div><div>- foreach state and a != a_o, \hatQ(s, a) = “terrible"</div><div><br/></div><div>random restarts (slow)</div><div><br/></div><div>Use simulated annealing-like approach: take a random action:</div><div><span style="line-height: 1.45;">     <img src="RL2%20-%20Reinforcement%20Learning.html.resources/AA90D8F7-13D2-46E6-87B4-616B44C7156A.gif" height="18" width="166"/> sometimes, with probability 1-epsilon</span><br/></div><div>this will learn and it will use what it learns</div><div>this is called GLIE - Greedy in the Limit with Infinite Exploration</div><div>- \hatQ —&gt; Q and \hat{\pi} —&gt; \pi^*</div><div>- not only are we learning, but we’re applying what we learn</div><div><br/></div><div><br/></div><div>exploration-exploitation dilemma - fundamental tradeoff in RL</div><div>- trying new and different things</div><div>- using what you learn is critical</div><div>- information must go </div><div>model-based - knows what you know</div><div><br/></div></body></html>
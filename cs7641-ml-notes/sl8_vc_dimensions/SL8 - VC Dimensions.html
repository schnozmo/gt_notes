<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Computational Learning Theory, Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-02-01 06:34:10 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-02-11 23:41:14 +0000"/><title>SL8 - VC Dimensions</title></head><body><div><span>    Infinite Hypothesis Spaces</span><br/></div><div><span><br/></span></div><div><span><img src="SL8%20-%20VC%20Dimensions.html.resources/02C40EFC-3A74-4150-A1C2-CD9064DEFD49.png" height="120" width="415"/><br/></span></div><div>if the hypothesis space is really big, required sample size is infinite</div><div><br/></div><div>Infinite hypothesis spaces</div><div>- linear separators</div><div>- artificial neural networks</div><div>- decision trees (continuous inputs) - as long as there aren’t infinite splits on the same inputs</div><div><br/></div><div>X: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}</div><div>H: h(x) &gt; theta, which is a member of R</div><div><br/></div><div>In this case |H| is infinite, so we can’t track all hypothesis. <b>But</b>, we could track non-negative integers ten or below, because they all result in the same answer</div><div><br/></div><div>Syntactic hypothesis space - all hypotheses you could write</div><div>Semantic hypothesis space - only hypotheses that are meaningfully different</div><div><br/></div><div>Power of a hypothesis - what is the largest set of inputs that the hypothesis class can label in <u>all possible ways</u> =&gt; 1</div><div>S = { 6 }</div><div><br/></div><div>if H is h(x) &gt; theta, theta represents a point on the number line. Any x on or to the right of the number line will be T, anything to the left will be F. 1 X can be labelled in 2 ways [T,F]. 2 X can be labelled in 3 ways [TT, FT, FF]. (NB - can’t be labelled TF because any T instance left of theta must have the second value T also)</div><div><br/></div><div>A set of instances S is <u>shattered</u> by H iff for every dichotomy of S there exists some hypothesis in H consistent with this dichotomy</div><div><br/></div><div>Example - 3 instances yields 8 hypotheses</div><div><br/></div><div>FFF</div><div>FFT</div><div>FTF</div><div>TFF</div><div>FTT</div><div>TFT</div><div>TTF</div><div>TTT</div><div><br/></div><div>The Vapnik-Chervonenkis dimension of hypothesis space H defined over instance space X is the size of the <u>largest finite subset</u> of X “shattered” by H. If arbitrarily large finite sets of X can be shattered by H, then VC(H) = inf. For any finite H, VC(H) &lt;= log_2|H|.</div><div><br/></div><div>To prove that a VC dimension exists at a certain level - need to show that there exists a set of points such that, for all labelings, there exists at least 1 hypothesis</div><div><br/></div><div>To prove that a VC dimension does not exist at a certain level - need to show that for all points there exists a labelling such that there is no hypothesis</div><div><br/></div><div><img src="SL8%20-%20VC%20Dimensions.html.resources/226813A7-C463-45A0-8685-5EE381F532FA.png" height="510" width="830"/><br/></div><div><br/></div><div><br/></div><div>VC dimension is <u>often</u> equal to the number of parameters</div><div><br/></div><div>Sample complexity for infinite H.</div><div><img src="SL8%20-%20VC%20Dimensions.html.resources/F57164D8-FF3C-4991-8FFE-D2B391C47278.png" height="135" width="645"/><br/></div><div>Theorem: H is PAC-learnable iff VC(H) is finite</div><div><br/></div></body></html>
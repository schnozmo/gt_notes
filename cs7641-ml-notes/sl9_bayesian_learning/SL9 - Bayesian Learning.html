<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Bayes, Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-02-12 00:19:28 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-03-18 20:13:20 +0000"/><title>SL9 - Bayesian Learning</title></head><body><div>ML - “Learn the <u>Best</u> hypothesis given data (and some domain knowledge)”</div><div><br/></div><div>more precise about <u>best</u> =&gt; <u>most probable</u> or most probably correct</div><div><br/></div><div>h = hypo</div><div>D = data</div><div><br/></div><div>argmax_h_subset_of_H P( h | D )</div><div><br/></div><div>Bayes rule =&gt; P(h | D) = ( P( D | h ) * P( h ) ) / P( D )</div><div><br/></div><div>P( D ) =&gt; prior belief of the data, really it is a normalizing term</div><div>P( D | h ) =&gt; likelihood we would see this data given the hypothesis. The data is our training data, x_i, d_i (or label). So, this is the probability that, given these x_i’s, we would get these d_i’s. This is accuracy of hypothesis</div><div>P( h ) =&gt; prior belief of this hypothesis (domain knowledge), a priori</div><div><br/></div><div>left side is tough to figure out, but right side is more concrete</div><div><br/></div><div>Bayesian learning:</div><div><br/></div><div>foreach h member of H,</div><div>   calc P( h | D ) = P( D | h ) * P ( h ) ##   ignored / P ( D ) since all h will be divided by the same P( D ) and we need only need to find maximum h, not the value of the maximum h </div><div>   output h_max = argmax P ( h | D )</div><div><br/></div><div>MAP = Maximum a posteriori</div><div><br/></div><div>If we don’t have good domain knowledge (i.e., the P( h )), then we are maximizing P( D | h ), this is the ML, maximum likelihood.</div><div><br/></div><div>These are necessarily practical, since we still need to evaluate every hypothesis</div><div><br/></div><div>Example:</div><div>1 - given { x_i, d_i } as (noise free) examples of c</div><div>2 - c is a member of H</div><div>3 - uniform prior, i.e., P( h )</div><div><br/></div><div>P( h ) = 1/| H |</div><div>P( D | h ) = { 1 if d_i = h(x_i) for all x_i, d_i member of D, 0 otherwise }</div><div>P( D ) = sum( P( h ) * P( D | h ) )</div><div><br/></div><div>given data, the probability of a particular hypothesis of being the best is uniform across those hypotheses consistent with the data</div><div>   (VS -&gt; version space, set of hypotheses consistent with the data)</div><div><br/></div><div>So, if we don’t know anything about the priors and the data are not noisy, pick any hypothesis from the version space</div><div><br/></div><div><img src="SL9%20-%20Bayesian%20Learning.html.resources/4FB3240B-C63D-4565-9737-2E59278229D3.png" height="593" width="1050"/><br/></div><div><br/></div><div><br/></div><div>with noisy data</div><div>1 - given { &lt;x_i, d_i&gt; }</div><div>2 - d_i = f( x_i ) + epsilon_i  ##  epsilon_i is an error value</div><div>3 - epsilon_i ~ N( 0, sigma**2 )  ## epsilon_i drawn from a (N)ormal distribution with mean 0 and sigma**2 variance</div><div><br/></div><div>what is max likelihood hypothesis?</div><div>argmax P( h | D ), which we saw before was the same as </div><div>argmax P( D | h ). We know this is the product of the individual probabilities of getting the right label given hypothesis</div><div>argmax product( P( d_i | h ) ), given h is true (consistent hypothesis). h is really an f(x)</div><div><span><br/></span></div><div><span>if h is true, that means f(x_i) = d_i plus an error drawn from a normal distribution such that the error is relatively small. This is a </span><span>Gaussian </span><span>function. The definition of a Gaussian is:</span></div><div><span><span>    <img src="SL9%20-%20Bayesian%20Learning.html.resources/B1B65372-5270-44E2-9CF8-1DBAC2ED4F9A.png" height="124" width="331"/><br/></span><br/></span></div><div><span><br/></span></div><div><span>So, our formula is:</span></div><div><span><img src="SL9%20-%20Bayesian%20Learning.html.resources/38B7C5AB-D75F-41A8-9AFB-AD04BF3CA0FC.png" height="157" width="563"/><br/></span></div><div><span>To simplify, we are doing an argmax and 1/sqrt( 2*pi*sigma**2 ) doesn</span>’<span>t depend on i, we can eliminate that term. taking natural log of both sides and log of products = sum of logs</span></div><div><span><br/></span></div><div><span><img src="SL9%20-%20Bayesian%20Learning.html.resources/4E452A4E-F99A-44D5-A8D3-3CF9C3BF222B.png" height="135" width="619"/><br/></span></div><div><span>1/2 and sigma**2 can be eliminated:</span></div><div><span><img src="SL9%20-%20Bayesian%20Learning.html.resources/112812F8-CF4A-493B-99BC-DB602E5BC123.png" height="157" width="497"/><br/></span></div><div><span>which is the sum of squared error. So the maximum likelihood hypothesis is the one that minimizes the sum of squared error. This is still based on the assumption that the error is drawn from a normal distribution. If error is from another noise model, this doesn</span>’<span>t work. Also, we assume our x_i</span>’<span>s are not noisy</span><br/></div><div><span><br/></span></div><div><span><br/></span></div><div><br/></div><div>h_max = argmax[ P( D | h ) * P ( h ) ]. log, because it turns products into sums makes things easier. Also, log is monotonic, meaning it doesn’t change the order of any results. taking log_2 of the right:</div><div><br/></div><div>h_max = argmax[ lg( P( D | h ) + lg( P( h ) ) ]   ### by mult by -1, change to argmin:</div><div><br/></div><div>h_max = argmin[ -lg( P( D | h ) - lg( P( h ) ) ]   ###  info theory, optimal code of event w with probability P has length -lg( P )</div><div>                                           where -lg( P( D | h ) is roughly the length of the data given h - if the hypothesis describes the data well, little data</div><div/><div><div/></div><div>                                                                          will be needed to describe the hypothesis. the more data that’s needed, the more variance it</div><div>                                                                          contains. variance = misclassification or error</div><div>                                                      -lg( P( h ) ) is roughly the length of the hypothesis - number of bits needed to describe the hypothesis,</div><div>                                                                       which means that we are trying to minimize this (which is kind of like pruning a decision tree)</div><div><br/></div><div>so the best hypothesis is the one that minimizes (1) error and (2) the size of the hypothesis. these are tradeoffs and error may have a different measurement than hypothesis size.</div><div><br/></div><div>This value is the <u>minimum description length</u></div><div><br/></div><div>for the best <u>hypothesis</u>, compute foreach h member of H, P( h | D ) and output argmax</div><div>for the best <u>label</u>, Bayes Optimal Classifier, need weighted vote foreach h member of H, using P( h | D ) as h’s weighted vote</div><div>algorithm for the best label is</div><div><br/></div><div/><div><img src="SL9%20-%20Bayesian%20Learning.html.resources/5A1EA6FA-F018-4D6D-8AAB-BD77854AF46A.png" height="39" width="308"/><br/></div><div>Summary</div><div>- Bayes’ rule - </div><div>- prior hypothesis P( h ) matter</div><div>- Maximum a posteriori hypothesis</div><div>- Maximum likelihood - uniform priors</div><div>- Minimum description length</div><div><br/></div></body></html>
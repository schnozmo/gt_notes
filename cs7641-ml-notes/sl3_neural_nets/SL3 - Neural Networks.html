<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Neural Networks, Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-01-19 05:14:22 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-01-19 05:37:17 +0000"/><title>SL3 - Neural Networks</title></head><body><div>Artificial Neurons</div><div><br/></div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/D93D4169-700E-4ACC-88EC-737F21173B20.png" height="511" width="803"/>y will = 0<br/></div><div><br/></div><div>if activation &gt; threshold, y = 1, else y = 0</div><div><br/></div><div>Given 2d space and weights, what is the decision line?</div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/D899A5CB-FCB8-4164-ABC3-52475442E37B.png" height="591" width="954"/><br/></div><div><br/></div><div>Combination of AND, OR, and NOT can yield any boolean function</div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/265C7C74-6B21-4CD5-ACD1-EB89158292EC.png" height="513" width="668"/><br/></div><div><br/></div><div>Given examples, find weights that map inputs to outputs</div><div>- perceptron rules makes use of thresholded values</div><div>- gradient descent/delta rule makes use of unthresholded values</div><div><br/></div><div>Perceptron Rule - single unit</div><div><img src="SL3%20-%20Neural%20Networks.html.resources/CA49E43C-8B4B-44F6-B019-FD88D426FACA.png" height="609" width="1095"/><br/></div><div>1 - if half-plane exists that separates different outputs, called linearly separable. If linearly separable, perceptron rule <u>will find</u> w’s in finite resources</div><div>2 - perceptron rule is non-differentiable, y = 0 for x &lt;= 0 and y = 1 for x &gt; 0. discontinuity at x = 0 means function not differentiable</div><div><br/></div><div>Gradient Descent  - robust to problems that are not linearly-separable</div><div>- output not thresholded, i.e., evaluates activation</div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/369F48F8-47D5-40FA-A6C6-5E8FE4B722E8.png" height="594" width="1005"/><br/></div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/BB8F9A57-693F-4EB6-90D5-26090C04C4DF.png" height="276" width="947"/><br/></div><div><br/></div><div>Sigmoid - applied to activation</div><div><br/></div><div>sigma | a = 1/(1 + e**-a)</div><div>- as a -&gt; -inf, sigma(a) -&gt; 0</div><div>- as a -&gt; inf, sigma(a) -&gt; 1</div><div><br/></div><div>Now, we can use sigmoid in perceptron rule (there are other functions, but this is a nice intro)</div><div><br/></div><div><img src="SL3%20-%20Neural%20Networks.html.resources/BAFB09E0-EC1A-41DB-8A04-6E89E8E33DD7.png" height="567" width="1035"/><br/></div><div>If  whole neural network is differentiable, we can tell how much adjust weights to tell how they affect the output</div><div><br/></div><div>Backpropagation - computationally benefictial organization of the chain rule error communication reverse direction of the network</div><div><br/></div><div>Many local optima - can’t change weights without making error worse</div><div><br/></div><div>Optimizing weights</div><div>- gradient descent</div><div>- advanced methods</div><div><span>    - momentum</span><br/></div><div><span><span>    - higher order derivatives (careful of overfitting)</span><br/></span></div><div><span><span><span>    - randomized optimization</span><br/></span></span></div><div><span><span><span><span>    - penalty for “complexity” - too many nodes, too many layers, too high weights</span><br/></span></span></span></div><div><span><span><span><span><br/></span></span></span></span></div><div><span><span><span><span>Restriction Bias - set of hypotheses we will consider, representational power</span></span></span></span></div><div>- Boolean - network of threshold-like functions</div><div>- Continuous - connected, no jumps - only 1 hidden layer needed</div><div>- Arbitrary - stitched together - 2 hidden layers</div><div><br/></div><div>ANN not restrictive bias, but risk of overfit</div><div>- use cross-validation - to determine node count, layer count - helps to find training halt point</div><div>- use bounds for max node count and layer count to keep n/ws from being overly complex</div><div><br/></div><div>Performance Bias - algorithm selection of one representation over another</div><div>- how to select initial weights</div><div><span>    </span>- small random numbers</div><div><span>    <span>    - random such that different initial weights in many runs may help to avoid local minima</span></span><br/></div><div><span><span><span>    <span>    - small to keep low “complexity"</span></span><br/></span></span></div><div><br/></div></body></html>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-01-19 05:37:36 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-01-22 18:14:48 +0000"/><title>SL4 - Instance Based Learning</title></head><body><div><img src="SL4%20-%20Instance%20Based%20Learning.html.resources/15176B38-E36F-47EA-BD89-48F85CD1159B.png" height="611" width="953"/><br/></div><div><br/></div><div><span>   </span><br/></div><div><img src="SL4%20-%20Instance%20Based%20Learning.html.resources/D4DCCA46-BD54-47EA-8DF6-775285CE1097.png" height="597" width="816"/><br/></div><div>For classification, yi is discrete, return plurality vote (most common value)</div><div>For regression, yi is real, return mean of yi</div><div><br/></div><div>for ties in distances, take &gt;=k</div><div><br/></div><div>KNN is learning easy and query expensive, while linear regression is learning expensive and query cheap</div><div><img src="SL4%20-%20Instance%20Based%20Learning.html.resources/05C09F24-0233-406C-9541-65CAAF036D34.png" height="510" width="899"/><br/></div><div><br/></div><div>KNN preference bias</div><div>+ locality -&gt; near points are similar</div><div>+ smoothness -&gt; averaging</div><div>+ all features matter equally</div><div><br/></div><div>Curse of Dimensionality - as the number of features or dimensions grows, the amount of data we need to generalize accurately grows <u>exponentially</u></div><div><u><br/></u></div><div/><div>Distance </div><div>- Euclidean or Manhattan</div><div>- in general, similarity</div><div><br/></div><div>selecting k - weighted distance</div><div><br/></div><div>locally weighted regression - replacing average with a more general function</div><div><br/></div><div><font style="font-size: 18px;"><b>Mitchell Chapter 8</b></font></div><div><br/></div><div>Nearest Neighbor</div><div>Locally weighted regression</div><div>- training data is not processed, but stored</div><div>- new data is compared with training data to find similar instances</div><div><span>    - remembers</span><br/></div><div><span>    - training data can change over time?</span><br/></div><div><span><span>    - advantage when target function complex, described by a less complex series of local approximations</span><br/></span></div><div><span><span><span>    - long classification time</span><br/></span></span></div><div><span><span><span><span>    - classification looks at all attributes, where only some SL types favor important attributes (DLT)</span><br/></span></span></span></div><div><span><span><span><span><br/></span></span></span></span></div><div><span><span><span><span>8.2 K-Nearest Neighbor</span></span></span></span></div><div><span><span><span><span>Instance X</span></span></span></span></div><div><span><span><span><span>feature vector a1(x), a2(x), …, an(x)</span></span></span></span></div><div>distance between 2 instances x,j = d(xi.xj) = SQRT( SUM(ar(xi) - ar(xj))**2 )</div><div><br/></div><div>target function can be discrete or real-valued</div><div><u>discrete-valued functions</u> take most common value</div><div><br/></div><div>Training algorithm -&gt; add example x, f(x) to <u>list</u></div><div><u><br/></u></div><div>Classification algorithm</div><div>- Given query xq to be classified</div><div>- x1…xk are the k instances from <u>list</u> nearest to xq</div><div>- return f^(xq) &lt;- argmax(  SUM(  sig(v, f(xi)  ), wher sig(a, b) = 1 if A = B, otherwise sig(a, b) = 0</div><div><br/></div><div><u>real-valued functions</u> -&gt; take average of nearest instances</div><div><br/></div><div>f^(xq) = SUM(  f(xi)  )/k</div><div><br/></div><div>Distance-weighted nearest neighbor</div><div>- weight the contribution of each nearest neighbor according to their distance from the query point</div><div><span>    - common weight is the inverse square of distance from xi - xq</span><br/></div><div><span>- if xi == xq, f^(x) -&gt; f(x)</span></div><div><span><br/></span></div><div><span><u>discrete</u></span></div><div><span>f^(xq) &lt;- argmax(  SUM(  wi * sig(v, f(xi))  )  ), where wi = 1/(d(xi.xq)**2)</span></div><div><br/></div><div><u>real</u></div><div>f^(xq) &lt;- SUM(  wi * f(xi)  ) / SUM( wi )</div><div><br/></div><div>we could add distance weighting to all instances (not just closest), since distant xi will contribute very little, but classification will be even slower</div><div><br/></div><div>KNN</div><div>- robust to noise</div><div>- effective for large data sets</div><div>- inductive bias<span> - assumption that classification of query points will be most similar to those at nearby points</span></div><div><span>- uses all attributes</span></div><div><span><span>    - DTL uses those with most info gain</span><br/></span></div><div><span>    - curse of dimensionality - too many features</span><br/></div><div><span><span>    <span>    - weight attributes to avoid curse</span></span><br/></span></div><div><span><span><span><span>    <span>    <span>    - use cross-validation to determine weights</span></span></span><br/></span></span></span></div><div><span><span><span><span><span><span><span>    <span>    <span>    - leave-one-attribute-out cross-validations</span></span></span><br/></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>Efficient indexing of training data</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- kd-tree - instances stored in leaves, nodes put xq down the relevant path based on attributes</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>8.3 Locally Weighted Regression</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- generalization of kNN</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- explicit approximation to f surrounding xq</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- uses nearby or distance-weighted training examples to form f^</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>can use</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- only kNN</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- distance-weighted all instances</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>- distance-weighted kNN</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>8.4 Radial Basis Functions</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>f^(x) = w0 + SUM(  wu * ku(  d(xu, x)  )  )</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>xu is instance of X</span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>ku(  d(xu, x)  ) is weighted kernel function which decreases as d(xu, x) increases</span></span></span></span></span></span></span></span></span></div><div>K - user-provided constant that specifies number of kernel functions to be included</div><div><br/></div></body></html>
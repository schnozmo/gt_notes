<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Reinforcement Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-04-05 02:43:02 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-04-08 18:31:18 +0000"/><title>RL1 - Markov Decision Processes</title></head><body><div>Supervised Learning:       y = f(x)          - function approximation</div><div>Unsupervised Learning:         f(x)          - find a description</div><div>Reinforcement Learning: y = f(x), z       - given x and z’s, find f to generate y</div><div><br/></div><div>Chapter 17 of <a href="http://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach</a><br/></div><div><br/></div><div><img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/7F9C7187-9F59-4EFF-9B6D-CCD71D80D118.png" height="487" width="885"/><br/></div><div><br/></div><div><br/></div><div><b>Markov Decision Processes</b></div><div><br/></div><div><br/></div><div><b>States</b> - current grid location; 12 in this example (realy 11)</div><div><span><b>Model/Transition function</b> - rules of the world, function of 3 variables - state, action, new state gives the probability of getting to a new state, given a start state and an action</span><br/></div><div><b>Actions</b> - things you can do in a particular state - up, down, left, right</div><div><b>Reward</b> - scalar value for being in a state - usefulness of being in a state, domain knowledge</div><div><br/></div><div>Markovian property - only the present matters - transition function only depends on current state. assuming transition function is stationary</div><div><br/></div><div><b>Policy</b> - the solution to a MDP. Given a state, return an action. Optimal policy that maximizes long-term expected reward</div><div><br/></div><div>for MDPs (and RL), training is given as &lt;s, a, r&gt; triples. MDP policy tells you what action to take given a state, not a sequence of actions.</div><div><img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/21481BBF-0064-4232-AECB-7C7B7A0242A4.png" height="414" width="725"/><br/></div><div><br/></div><div>Delayed reward - don’t know after one move if it was good or not. This is a temporal credit assignment problem;</div><div> </div><div><br/></div><div>we’ve been assuming "Infinite horizons” - there’s no clock/countdown. with finite horizon, policy may change even though we’re in the same state</div><div><br/></div><div><div/><div>Utility of sequences - if U(s0, s1, s2, …) &gt; U(s0, s1’, s2’, …), then U(s1, s2, …) &gt; U(s1’, s2’, …)</div><div/><div>     <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/06AB030C-C52A-4045-9A0A-3153BF5EBFE9.gif" height="38" width="183"/>, but this is unrealistic, because we could potentially just run things until infinity, where all policies end up at the same</div><div><br/></div><div>Slight change is to introduce gamma at t as a factor, where gamma ge 0 and lt 1 - discounted reward </div><div>       <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/A5FCE2DB-5A47-46AB-B3B0-08166AB5B978.gif" height="50" width="192"/><br/></div><div>Bound from above using max reward / 1 - gamma</div><div>    <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/9B4CBE28-F45C-4D74-8BC7-02173DDE2467.gif" height="41" width="42"/></div><div>Discounted reward ==&gt; geometric series</div><div>Allows you to see a looooong way away</div><div><br/></div><div>Singularity - limit to computer power is time to design next gen of computer. If computer designs, with mores law, takes half the time. Until takes no time.</div><div><br/></div><div><img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/A6F33CBA-B91D-4DDA-BFBD-9E2CDAAB32F6.png" height="504" width="896"/><br/></div><div><br/></div><div>E is expectation</div><div><br/></div><div>Best policy is the one that max long term expected reward</div><div><br/></div><div>Utility of a state depends on policy being followed, and the state we're currently at </div><div>Reward at state ne utility at state. Short term vs long term. OMS now is neg reward but high utility</div><div><br/></div><div>Best <span style="-webkit-text-size-adjust: 100%;">policy at a state is that at every state returns the action that maximizes the expected reward wrt the opt policy</span></div><div><span style="-webkit-text-size-adjust: 100%;"><br/></span></div><div><span style="-webkit-text-size-adjust: 100%;">True utility for a state is the reward at that state plus discount of the future rewards. Last is the bellman equation, which is the key for solving MDPs</span></div><div><span style="-webkit-text-size-adjust: 100%;"><br/></span></div><div><span style="-webkit-text-size-adjust: 100%;"><img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/69E22852-FD86-42B6-A853-9FD7017233DE.png" height="583" width="1063"/><br/></span></div><div><br/></div><div><b>Solving Bellman through value iteration</b></div><div><br/></div><div>     <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/586769B0-137E-4301-AF04-DA0AA9FBC7A3.gif" height="39" width="388"/><br/></div></div><div>there are n different values of s. the unknowns are the n values of s and there are n equations of U(s). We could solve for s, but the max makes it problematic</div><div><br/></div><div>- Start with random, arbitrary utilities</div><div>- Update utilities for a state based on all neighbor states it can reach</div><div>- Repeat until convergence<br/></div><div><br/>using all Us at previous timestep to update Us at current timesstep. The rewards will propagate forward and back across timesteps. These Rs are </div><div><br/></div><div>U’s don’t have to be correct, policy maps states to optimal next action. U’s only have to be in the correct order such that the right action is taken from each state</div><div><br/></div><div><b>Finding Policies - policy iteration</b></div><div><br/></div><div>- start with pi_0 &lt;- guess</div><div>- evaluate: </div><div><span>    <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/F496C440-D093-48E1-B787-667D7E516148.gif" height="18" width="63"/></span><br/></div><div>- improve:</div><div><span>    <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/22D5D170-57EA-403F-BE30-CBE0787C6B2B.gif" height="26" width="290"/>, where <img src="RL1%20-%20Markov%20Decision%20Processes.html.resources/149A8B7F-7624-412E-A934-F809A3139C26.gif" height="39" width="351"/></span><br/></div><div><span><br/></span></div><div><span>since there’s no argmax, this is a set of linear equations</span></div><div><br/></div><div>for MDPs, we know the states, actions, rewards, actions, transitions, etc</div><div>for Reinforcement learning, we don’t know a lot or any of these items</div><div><br/></div><div><br/></div></body></html>
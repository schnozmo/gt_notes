<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="created" content="2017-04-19 21:05:59 +0000"/><meta name="source" content="mobile.ipad"/><meta name="updated" content="2017-04-30 01:51:29 +0000"/><title>RL3 - Game Theory</title></head><body><div>Mathematics of conflict/conflicts of interest</div><div>- single agents --&gt; multiple agents</div><div>- game theory comes out of economics</div><div>- increasingly a part of AI/ML</div><div><br/></div><div>Simple game</div><div><br/></div><div>States, actions</div><div><br/></div><div>A can go left or right</div><div>- A goes left, B can go left (+7), middle (+3) or right</div><div>   - if B goes right, A could go left (-2) or right (+4) </div><div>- A goes right, B could only go right (+2)</div><div>2-player,</div><div>Zero sum,      - A's reward is -1 * B's reward</div><div>Finite,              - small number of states</div><div>Deterministic,       - no random movements</div><div>Perfect information.     - players know their state</div><div><br/></div><div>Strategy - mapping for each player of states to actions</div><div>Pure strategies</div><div><br/></div><div>Strategies for A</div><div>L, L</div><div>L, R</div><div>R, L</div><div>R, R</div><div><br/></div><div>Strategies for B</div><div>L, M, R</div><div>R, R, R</div><div><br/></div><div>Matrix form of the game</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/Evernote%20Snapshot%2020170419%20172813.jpg" height="1455" width="1947"/><br/></div><div><br/></div><div>This represents all the strategies available to both players and contains all the information necessary for the players to select strategies</div><div><br/></div><div>A can't just attempt to maximize (+7), because B can select a strategy that avoids A maximum. A and B must consider the worst case/counter the opponent's maximum when selecting strategies. A (since it goes first), is selecting the max min and B is selecting the min max - this is MINIMAX. If A and B both do the 'rational' thing, the value of the game will be 3 for A. It will not go right, because 2 is the max. B will force middle, because A has control to get 4 if it tries for the -1. This might not be the same for perfect information.</div><div><br/></div><div>Theorem - In a 2player zero-sum deterministic game of perfect information, MINIMAX = MAXIMIN - and there always exists an optimal pure strategy for each player.</div><div><br/></div><div>Assumption - every player is trying to maximize rewards and knows that every other player is trying to do the same thing. </div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/13FEB06E-629B-4AF0-A5D5-0F04DFEB373B.png" height="463" width="833"/><br/></div><div><br/></div><div><br/></div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/B167BEC3-FFEB-4959-A17D-A64E5F822D05.png" height="600" width="1108"/><br/></div><div><br/></div><div>B doesn’t know, based on A holding, what state he’s in.</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/377BC6B6-9D66-4FA7-B0BB-91EF50DA5DAC.png" height="268" width="411"/><br/></div><div>Moving to hidden information, the von Neumann theorem doesn’t hold, so MINIMAX isn’t necessarily equal to MAXIMIN. There is no pure strategy to employ, but a MIXED strategy. A mixed strategy is just a distribution over strategies. P is the probability of A being a holder</div><div><img src="RL3%20-%20Game%20Theory.html.resources/E09A41CF-E0B8-4C0B-B940-6C975B7E46E4.png" height="401" width="363"/><br/></div><div><br/></div><div>These are both linear equations, where they are equal (15p - 5 = -10p + 5, p = 0.4), we can find the value of the game at p = 0.4, which is -10(0.4) + 5 = 1</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/8D2D9E34-48CB-4450-BBCF-8128F8E5A9DB.png" height="357" width="565"/><br/></div><div><br/></div><div>2-player, non-zero-sum, non-deterministic game of hidden information - “Snitch”- 2 criminals asked to rat on the other</div><div><br/></div><div>each criminal A, B are asked to incriminate the other for the benefit of walking. If one defects and the other doesn’t, the defector walks and the other gets a 9 month sentence. If they both defect, each gets 6 months. If they both keep their mouths shut, they each get 1 month.</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/11E8C62F-7B0E-4928-AD00-7B5ED24E5EE3.png" height="264" width="495"/><br/></div><div><br/></div><div><br/></div><div>Nash equilibrium</div><div>- N players with strategies S_1, S_2, S_n</div><div>- S_1*, S_2*, S_N* is in a Nash Equilibrium iff, every player has no reason to change their strategies</div><div>- </div><div><br/></div><div>Strictly dominated strategy - a strategy that you would always move away from</div><div>"<span>If a player has a dominant <b>strategy</b> than all others are <b>dominated</b>, but the converse is not always true. A <b>strictly</b> dominant <b>strategy</b> is always played in equilibrium, and thus <b>strictly dominated strategies</b> never are. For example, in the prisoner's dilemma, each player has a <b>dominated strategy</b>."</span></div><div><br/></div><div>- in the n-player pure strategy game, if elimination of strictly dominated strategies eliminates all but one combination, that combination is the unique Nash Equilibrium</div><div>- Any Nash Equilibrium will survive elimination of strictly dominated strategies</div><div>- if n is finite and for all i, S_i is finite: there exists a mixed Nash Equilibrium</div><div><br/></div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/70F3B676-E162-4BD3-BA61-270EFC5E4D0C.png" height="487" width="858"/><br/></div><div><br/></div><div><div><h1><font style="font-size: 18px;">What Have We Learned?
</font></h1><div><div><p><a href="http://www.autonlab.org/tutorials/gametheory.html">Andrew Moore's slides on Zero-Sum Games</a></p><p><a href="http://www.autonlab.org/tutorials/nonzerosum.html">Andrew Moore's slides on Non-Zero-Sum Games</a></p></div></div></div><div/><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/6A0AD5F2-0077-4E93-B98B-7AF3AFB3E77A.png" height="626" width="1006"/><br/></div><div><br/></div><div>Mechanism Design - changing the inputs/payments to induce specific outcomes</div><div><br/></div><div><br/></div><div><b><font style="font-size: 24px;">Game Theory 2</font></b></div><div><b><br/></b></div><div><b><br/></b></div><div>In a multi-round prisoner’s dilemma, the last round is always the Nash Equilibrium and the preceeding rounds are the same. But what if the number of rounds left is unknown?</div><div><br/></div><div>with probability gamma, the game continues. Each round could be the last with probability 1-gamma. Gamma here is just like the discount factor in single agent RL</div><div><br/></div><div>There are a finite number of rounds if gamma &lt; 1</div><div><br/></div><div>Tit for Tat strategy</div><div>- cooperate with prisoner on first round</div><div>- copy other prisoner’s previous move therafter</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/6599F7CF-E257-4BBC-860B-8A8FE7A6CF31.png" height="535" width="1073"/><br/></div><div><br/></div><div>Mutual best response - pair of strategies where each is the best response to the other. This results in a Nash Equilibrium</div><div><br/></div><div><br/></div><div>Repeated games and the Folk Theorem</div><div> - general idea - in repeated games, the possibility of retaliation opens the door for cooperation</div><div>- in mathematics, a folk theorem is when results that are known, at least to experts in the field, and considered to have established status, but not published in complete form. (Oral tradition)</div><div>- In game theory - a folk theorem refers to a particular result: it describes the set of payoffs that can result from Nash strategies in <u>repeated</u> games.</div><div>- in repeated games, the possibility of retailiation opens the door for cooperation</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/D30BA2CB-83CC-4311-A63A-E5EFB6EB0EF9.png" height="516" width="859"/><br/></div><div><br/></div><div>Minmax/security level profile</div><div><br/></div><div>pair of payoffs, one for each player, that represent the payoffs that can be achieved by a player defending itself from a malicious adversary (zero sum game) - (Bach, Stravinsky or Battle of the Sexes)</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/4B81593C-2D34-45F7-978B-7617EB3C57FB.png" height="524" width="947"/><br/></div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/000536EB-0A78-4149-A5AD-0B657759612A.png" height="542" width="1000"/><br/></div><div>Folk Theorem - Any feasible payoff profile that <u>strictly dominates</u> the minmax/security level profile can be realized as a Nash equilibrium payoff profile, with sufficiently large discount factor</div><div><br/></div><div>proof - if it strictly dominates the minmax profile, can use it as a threat. Better off doing what you are told!</div><div><br/></div><div>Grim Trigger</div><div>- start out at set of actions that produce mutual benefit. If one player crosses the line, then response is to retaliate forever</div><div>- in prisoner’s dilemma, start out cooperating until one person defects, then all players defect</div><div><br/></div><div>Implausible Threats</div><div>- <b>subgame perfect equilibrium: always best response independent of history</b></div><div>- Grim will respond to opponents (TfT) defect with defect, even though if Grim stayed with cooperate, it’d be better for Grim in the long run</div><div><br/></div><div>Pavlov machine</div><div>- start cooperating. cooperate if agree, defect if disagree</div><div>- pavlov vs pavlov is a Nash equilibrium, since there’s no reason for anyone to defect</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/77C4494E-7A31-459F-A948-E0B1E4094085.png" height="511" width="708"/><br/></div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/40EE93A4-B4EF-438B-B80D-85D5A996381D.png" height="594" width="728"/><br/></div><div><br/></div><div>Computational Folk Theorem</div><div>- Given any 2-player average reward, repeated game, can build a Pavlov-like machines and construct a subgame perfect Nash Equilibrium in polynomial time</div><div><span>    - Pavlov if possible</span></div><div><span/><span>    - else zero-sum-like (Solve a linear program)</span></div><div><span/><span>    - at most one player improves</span> </div><div><br/></div><div>Stochastic games</div><div>- MDP:RL::Stochastic Games:Mutliagent RL</div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/EA47D5FE-29F7-461E-B52D-C3C24CA26791.png" height="491" width="918"/><br/></div><div><br/></div><div><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/0997129C-9A2F-4F66-842C-D0ED81EF3444.png" height="620" width="1025"/><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/0D16545E-41E6-4D34-8BD6-7B3DD607A3A4.png" height="621" width="1050"/><br/></div><div><img src="RL3%20-%20Game%20Theory.html.resources/786B8215-BA9D-4736-86F7-CE80D5B2F0D2.png" height="552" width="934"/><br/></div><div><br/></div><div><br/></div><div>Deep Learning/Deep Neural Networks - new techniques for getting signal through multiple layers</div><div><br/></div><div>Big Data - algorithmic challenges from gigantic datasets. linear too slow</div><div><br/></div><div>Semi-supervised learning - extract info using unsupervised learning apply to supervised learning</div><div><br/></div><div>The Assumption of Balanced Labels - labels are ~50% distributed. in a problem where very few samples have a particular label, the best learner may not be able to predict any data points with that label. Clustered label</div><div><br/></div><div>TFIDF - Term Frequency Inverse Document Frequency - </div><div><br/></div><div>Cross Entropy</div><div><br/></div><div>POMDPs - Partially Observable Markov Decision Processes  - when states are not known</div><div><br/></div><div>Inverse RL - inducing reward functions based on observations of behavior</div><div><br/></div></body></html>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-04-03 03:53:48 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-04-03 03:54:14 +0000"/><meta name="application-data:corenote-localUUID" content="036A886A-D1B5-4E79-A389-B3D06B43BDB7"/><meta name="application-data:corenote-hostUUID" content="19C4C2F7-5279-4508-8B08-B507BD5A6C8C"/><title>Assignment 4 - Markov Decision Processes</title></head><body><div>Numbers
</div><div><h2/><p>The assignment is worth 15% of your final grade.
</p><h2>Why?
</h2><p>In some sense, we have spent the semester thinking about machine learning techniques for various forms of function approximation. It's now time to think about using what we've learned in order to allow an agent of some kind to act in the world more directly. This assignment asks you to consider the application of some of the techniques we've learned from reinforcement learning to making decisions.
</p><p>The same ground rules apply for programming languages as with the previous assignments.
</p><p><b><i>Read everything below carefully!</i></b></p><h2>The Problems Given to You
</h2><p>You are being asked to explore Markov Decision Processes (MDPs):
</p><ol><li><p>Come up with two interesting MDPs. Explain why they are interesting. They don't need to be overly complicated or directly grounded in a real situation, but it will be worthwhile if your MDPs are inspired by some process you are interested in or are familiar with. It's ok to keep it somewhat simple. For the purposes of this assignment, though, make sure one has a "small" number of states, and the other has a "large" number of states.
</p></li><li><p><span>Solve each MDP using value iteration as well as policy iteration. How many iterations does it take to converge? Which one converges faster? Why? Do they converge to the same answer? How did the number of states affect things, if at all?</span></p></li><li><p>Now pick your favorite reinforcement learning algorithm and use it to solve the two MDPs. How does it perform, especially in comparison to the cases above where you knew the model, rewards, and so on? What exploration strategies did you choose? Did some work better than others?<br/> 
</p></li></ol><p><b><span style="font-size: large;">Coding Resources<br/></span></b><span style="font-size: large;"><br/></span><span style="font-size: medium;"><a target="_blank" href="http://burlap.cs.brown.edu/">Brown-UMBC Reinforcement Learning and Planning (BURLAP) java code library</a><br/></span></p><h2>What to Turn In
</h2><p>You must submit a tar or zip file named <i>yourgtaccount</i>.{zip,tar,tar.gz} that contains a single folder or directory named <i>yourgtaccount</i> that in turn contains:
</p><ul><li>a file named <i>README.txt</i> that contains instructions for running your code</li><li>your code</li><li>a file named <i><span style="font-style: normal;">yourgtaccount-</span>analysis.pdf</i> that contains your writeup.</li><li>any supporting files you need</li></ul><p>The file yourgtaccount-<i>analysis</i>.pdf should contain:
</p><ul><li>A description of your MDPs and why they are interesting.</li><li>A discussion of your experiments.</li><li><b>Limit your analysis write-up to 10 pages.</b></li></ul><h2>Grading Criteria
</h2></div><div>As always you are being graded on your analysis more than anything else.
<br/></div><div><br/></div></body></html>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Unsupervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-02-18 04:48:31 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-02-27 02:22:41 +0000"/><title>UL1 - Randomized Optimization</title></head><body><div>Optimization</div><div>- input space X</div><div>- objective function/fitness function f:x —&gt; R</div><div>- Goal: find x* member of X such that f(x*) = max poss f(x) — like argmax</div><div><br/></div><div>route finding</div><div>neural networks - optimize weights, minimize error</div><div><br/></div><div>2 optimize problems</div><div><br/></div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/6895752F-CEBD-4DCE-8E6D-F8B50964108D.png" height="486" width="705"/><br/></div><div>Because the first has limited input, easy to calculate</div><div><br/></div><div><b>Optimization Approaches</b></div><div>- generate &amp; test - small input space, complex function</div><div>- calculus - function has derivative</div><div>- newton’s method - function has derivative, gradient descent finding derivative at many x to find x where derivative is 0, identifies 1 optimum (perhaps local and not global)</div><div><br/></div><div>bad signs - big input space, complex function, no derivative, hard to find derivative, many local optima =&gt; use Randomized Optimization!!</div><div><br/></div><div><br/></div><div><b>Hill Climbing</b> to find local optimum</div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/B94ED688-1160-419A-AAF8-C1810F3BA31D.png" height="508" width="885"/><br/></div><div>Example - guess a sequence of 5 bits - 10110</div><div>f(x) = number of bits correct</div><div>Neighborhood function - one-bit differences from X</div><div><br/></div><div>x1 - 00000  - score = 2</div><div><br/></div><div>neighborhood is:           new neighborhood is:      new neighborhood is:</div><div>10000 - score = 3*        11000 - score = 2</div><div>01000 - score = 1         10100 - score = 4*           11100 - score = 3</div><div>00100 - score = 3         10010 - score = 4            10110 - score = 5***********</div><div>00010 - score = 3         10001 - score = 2            10101 - score = 3</div><div>00001 - score = 1</div><div><br/></div><div><b>Random restart Hill Climbing</b></div><div>- once a local optimum reached, restart from a random x</div><div>- multiple tries to find a good starting place</div><div>- not much more expensive</div><div><br/></div><div><br/></div><div><br/></div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/B6BA3408-1E15-4A32-BAB1-B1737EFEB375.png" height="477" width="909"/><br/></div><div><br/></div><div>If, on average, it takes more function evaluations to find x* that there are x’s, not really efficient. However, if we’re dealing with R, there are an infinite number of X</div><div><br/></div><div>Randomized hillclimbing depends a lot on the size of the attraction basin [16, 19] in above.</div><div><br/></div><div><b>Simulated Annealing</b></div><div><br/></div><div>repeated heating and cooling strengthens the blade</div><div><br/></div><div>hillclimbing - always trying to exploit (believing data too much -&gt; overfitting)</div><div>sim anneal - don’t always improve, sometimes, you need to search (explore, believe data very little)</div><div><br/></div><div>Balanced between how much to believe the process</div><div><br/></div><div>Metropolis-Hastings algorithm</div><div><br/></div><div>for a finite set of iterations:</div><div>- sample new point x_t from N( x )</div><div>- jump to a new sample with probability given by an acceptance probability function - P( x, x_t, T )</div><div>- decrease temperature T, which is always &gt; 0</div><div><br/></div><div/><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/FE3919CD-90F7-46E0-950B-8A50D50322C0.gif" height="79" width="388"/><br/></div><div>so, move to x_t if it’s better than best known</div><div>if f( x_t ) a little smaller than f( x ) or T is very big, likely to make the jump</div><div>if (f x_t ) a lot smaller than f( x ) or T is small, likely not to make the jump</div><div><br/></div><div>T close to 0, like hill climbing</div><div>T close to inf, like a random walk</div><div><br/></div><div/><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/9729887E-5396-45AF-9726-243F8479B5C4.gif" height="27" width="174"/><br/></div><div><br/></div><div>Q: How to select T</div><div>Boltzmann</div><div><br/></div><div><b>Genetic Algorithms</b></div><div> </div><div>population of individuals</div><div>mutation &lt;- local search/Neighborhood of X</div><div>cross over &lt;- (new concept) improvement based on combined attribute improvement</div><div>generations &lt;- iterations of improvement</div><div><br/></div><div>without cross-over, this is much like randomized restart instead this can be done with || computing</div><div>with cross-over, populations can share learning </div><div>population represents a distribution</div><div><br/></div><div>P_0 - initial population of size K</div><div>Repeat until converged:</div><div>- compute fitness of all x member_of P_t</div><div>- select “most fit” individuals (could be top N, top %ile, weighted prob)</div><div><span>    - could incorporate temperature parameter, so that, early on, encourage exploring among all individuals, but as temp cools, restrict to most fit individuals</span><br/></div><div>- pair up individuals, replacing “least fit” individuals via/crossover and/or mutations</div><div><br/></div><div>Example</div><div><br/></div><div>X = 8-bit strings, with several fit individuals</div><div>2 fit individuals are crossed-over and produce offspring</div><div><br/></div><div>1 - <font color="#16b32d">0</font><font color="#9d3992">1</font><font color="#16b32d">0</font><font color="#9d3992">1</font><font color="#16b32d">0</font><font color="#9d3992">0</font><font color="#16b32d">1</font><font color="#9d3992">0</font></div><div>2 - <font color="#9d3992">0</font><font color="#16b32d">1</font><font color="#9d3992">0</font><font color="#16b32d">0</font><font color="#9d3992">1</font><font color="#16b32d">1</font><font color="#9d3992">1</font><font color="#16b32d">0</font></div><div>We must sample from these individuals to create 2 offspring using, e.g., even bits from 1 and odd from 2 for offspring A and vice versa for offspring B:</div><div>A - <font color="#9d3992">01011010</font></div><div>B - <font color="#16b32d">01000110</font></div><div><font color="#16b32d"><br/></font></div><div>one-point crossover - choose position and flip/flop, i.e., if we had chosen the half-way point and assigned the first half to one </div><div>uniform crossover - bit-flip at random bits - more like real offspring ;)</div><div><br/></div><div>bias - locality of bits matters, can independent subspaces be optimized (independent)</div><div>“second best solution to any problem”</div><div><br/></div><div>In randomized optimization, we’re not really capturing the history or structure, just finding the best point and we have an unclear probability distribution</div><div>- TABU search - remembering where we were</div><div><br/></div><div>Mitchell -</div><div><ul><li>GAs less likely to fall into local minima than gradient descent of ANN, because GD moves the hypothesis search smoothly, whereas the offspring of fit functions may be much different than their parents.</li><li>Crowding problem - One highly fit </li></ul></div><div>  </div><div><b>MIMIC</b> - <a href="http://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf">http://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf</a></div><div>- finding optima by estimating probability distributions</div><div>- directly model probability distribution, successively refine the model as we search through the space</div><div><span>    - if we can estimate the probabiliity in this way, it will convey the structure of the search space and the good points</span><br/></div><div><span><br/></span></div><div><span>theta is a threshold</span></div><div><span><img src="UL1%20-%20Randomized%20Optimization.html.resources/F27801FC-3338-4D06-B8F6-1DC0A2BE0EA6.png" height="475" width="611"/><br/></span></div><div>So, starting with theta_min, the distribution is uniform, but as we search/explore, we increase theta such that we eventually get to theta_max, which reveals an optima</div><div><br/></div><div>Repeat:</div><div>- generate samples from P( x )_theta_t      … population</div><div>- set theta_t+1 to nth percentile of population     … pick fittest</div><div>- retain only those samples such that f( x ) &gt;= theta_t+1</div><div>- estimate P( x )_theta_t+1    &lt;— this is the structure</div><div><br/></div><div><ol start="1"><li>we need to be able to estimate the new prob dist</li><li>the prob dist as theta increases needs to be similar to previous, but contain enough fit examples to estimate next prob dist</li></ol><div><br/></div><div>p( x ) = p( x_1 | x_2, x_3, …, x_n ) * p( x_2 | x_3, …, x_n ) * … * p( x_n )</div><div>X = [ X_1, X_2, X_3, …, X_n ]</div></div><div><br/></div><div>assumption - care only about dependency trees</div><div><br/></div><div>Dependency tree is a special Bayesian network where each node has exactly 1 parent, so every random variable depends on exactly one random variable, so can rewrite joint dist p( x ) as product( p( x_i | parent( x_i ) ). There is a minimal concept of relationships.</div><div><br/></div><div>How to derive dependency tree from sample (one example):</div><div><br/></div><div>Some true prob dist we care about - p</div><div><br/></div><div>estimate p^ dependent on the parent pi using the KL Divergence - <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a>, which measures the non-symmetric difference between 2 prob dist, or the information gained when one revises one’s beliefs from the prior (P^pi) to the posterior (p) or the amount of information lost when posterior estimates prior. </div><div><br/></div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/4252E6B7-7DA9-4126-BBE4-B92A1BCB7BD1.png" height="149" width="836"/><br/></div><div><br/></div><div>Our goal is to minimize the divergence. doing so means our candidate p^pi is a pretty close distribution to the true p</div><div><br/></div><div>Algebra simplifies to below, where</div><div>- h(p) is the entropy (p log p) of the true p</div><div>- sum( h( x_i | parent( x_i ) ) is the additional entropy of x_i given parent</div><div>- J_pi becomes a cost function (which we want to minimize!)</div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/96425040-BE15-4758-B251-E0EF44B85B12.png" height="172" width="715"/><br/></div><div><br/></div><div>In order to minimize, need to pick a set parent parents that give the most information about the offspring. More math stuff:</div><div><br/></div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/0173CBA7-C7F8-48B2-9E43-04338A7A6D2F.png" height="263" width="809"/> </div><div><br/></div><div>J’_pi turns out to be something called the “<a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a>” I, which we want to maximize. So, now we want parents which give the most information about the offspring. How do we find this? So, there’s actually a graph where every node is a feature in X and connected to every other node with an edge that represents the mutual information between nodes. Our job is to find the subtree of that graph that gives the most information, this is a maximum spanning tree. Use Prim maximum spanning tree algorithm which runs in polynomial time on dense graphs</div><div><br/></div><div>Practical matters of MIMIC</div><div>- does well with structure</div><div>- representing all values as we move from uniform PD to optima PD</div><div>- local optima</div><div>- time complexity - savings of orders of magnitude of iterations over other randomized optimizations, but each iteration takes longer in MIMIC - it works well when the cost of evaluating the fitness function f is high</div><div><br/></div><div><br/></div><div><img src="UL1%20-%20Randomized%20Optimization.html.resources/52E36E17-0106-4C1A-BDBD-82FF2536EB00.png" height="591" width="919"/><br/></div><div><br/></div></body></html>
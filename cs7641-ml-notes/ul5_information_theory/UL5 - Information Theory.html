<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Unsupervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-04-03 12:11:44 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-04-07 02:16:52 +0000"/><title>UL5 - Information Theory</title></head><body><div><br/></div><div>Every input and output vector in machine learning can be considered a prob density function</div><div><br/></div><div>Info theory</div><div><img src="UL5%20-%20Information%20Theory.html.resources/A56A2F1F-8049-48CA-89BB-9D5F2F5C3659.png" height="312" width="776"/><br/></div><div><br/></div><div>Claude Shannon (Bell Labs) - father of information theory</div><div><br/></div><div>10 coin flips</div><div>fair coin - HTHHTHTTHT - 5 heads, 5 tails - need 10 bits, uncertainty means we need to transmit every value</div><div>unfair coin - HHHHHHHHHH - 10 heads - no communication needed, no uncertainty</div><div><br/></div><div><br/></div><div><img src="UL5%20-%20Information%20Theory.html.resources/91982A67-43AB-4651-BDDC-CE288ACD2DED.png" height="421" width="750"/><br/></div><div>The sequence ABDAACDA fits the probabilities on the right. encoding the message, we get:</div><div><br/></div><div>0 110 10 0 0 111 10 0, which is 14 bits in length, so we save 2 bits over a static 2 bits per character language. The expected message length is 1.75 bits per character. this is variable length encoded </div><div><br/></div><div><img src="UL5%20-%20Information%20Theory.html.resources/562ED857-184C-424C-B5B4-99C8293AF5E8.gif" height="38" width="126"/> or <img src="UL5%20-%20Information%20Theory.html.resources/62EDAEAA-42B8-4CE0-8C0F-32234548ADDA.gif" height="38" width="141"/><br/></div><div><br/></div><div>joint entroy - joint probabilit of x and y</div><div><img src="UL5%20-%20Information%20Theory.html.resources/E24C2314-5DC1-4DFF-AF9B-A6A7E09B7B1D.png" height="91" width="511"/><br/></div><div><br/></div><div>conditional  entroy - measure of randomness of one variable given another</div><div><img src="UL5%20-%20Information%20Theory.html.resources/0C81DD30-BD14-4F23-9C0C-406FE94F4EDB.png" height="65" width="550"/><br/></div><div><br/></div><div>if X and Y are independent, then H(Y|X) = H(Y), y gets no information from x</div><div>otherwise H(X,Y) = H(X) + H(Y)</div><div><br/></div><div>Charles' notes on Information Theory can be found at <a target="_blank" href="http://www.robotvisions.org/4641/downloads/InfoTheory.fm.pdf">http://www.robotvisions.org/4641/downloads/InfoTheory.fm.pdf</a> .<br/></div><div><br/></div><div>mutual information</div><div><br/></div><div>I(X, Y) = H(Y) - H(Y  | X)</div><div><br/></div><div>2 Independent coints</div><div><img src="UL5%20-%20Information%20Theory.html.resources/03FBD77D-F058-4FDC-BC69-6DB4F88DA9DE.png" height="481" width="823"/><br/></div><div><br/></div><div>2 coins are independent, there is no mutual information between them</div><div><br/></div><div>2 Dependent coins - if A is heads, B is also heads. If A is tails, B is also tails</div><div><img src="UL5%20-%20Information%20Theory.html.resources/4BB2BA71-6662-4E78-B926-3C81A72B75F4.png" height="495" width="833"/><br/></div><div>random variable A gives some information to random variable B</div><div><br/></div><div>Kullback-Leibler Divergence,  KL divergence - distance between 2 distributions</div><div>always non-negative</div><div><br/></div><div><img src="UL5%20-%20Information%20Theory.html.resources/BFF38DEC-1375-4397-A505-BA1ADC3D7EC0.png" height="121" width="465"/><br/></div><div>In other words, it is the <a title="Expectation (statistics)" href="https://en.wikipedia.org/wiki/Expectation_%28statistics%29">expectation</a> of the <a title="Logarithmic difference (page does not exist)" href="https://en.wikipedia.org/w/index.php?title=Logarithmic_difference&amp;action=edit&amp;redlink=1">logarithmic difference</a> between the probabilities <i>P</i> and <i>Q</i>, where the expectation is taken using the probabilities <i>P</i><br/></div><div><br/></div><div><br/></div><div><a href="UL5%20-%20Information%20Theory.html.resources/info-theory(1).pdf">info-theory(1).pdf</a><br/></div><div><br/></div></body></html>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-01-13 04:19:21 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-01-19 05:01:22 +0000"/><title>SL1 - Decision Trees</title></head><body><div>2 types of supervised learning</div><div>- classification - X =&gt; discrete label</div><div>- regression - continuous value function</div><div><br/></div><div>Definitions</div><div>- Instances - vectors of attributes that define input (picture, credit score, etc)</div><div>- Concept - Function, mapping b/t input and output</div><div>- Target Concept - Answer we’re trying to find</div><div>- Hypothesis (Class) - set of concepts willing to entertain</div><div>- Sample - Training set</div><div>- Candidate - concept we’re considering as a target concept</div><div>- Testing Set - same format data as training set with no overlap</div><div><br/></div><div><br/></div><div>Dating example - you and a date are considering a restaurant. Should we pick that one or move along?</div><div><ul><li>features of restaurant - type of food served, atmosphere, occupancy, estimation of date’s thoughts, cost ($ or $$ or $$$ or $43)</li></ul><div><br/></div><div>Representation vs Algorithm</div><div><br/></div><div>DTs representation is a tree</div></div><div><br/></div><div><img src="SL1%20-%20Decision%20Trees.html.resources/ECF27361-630F-4F43-84DA-D4E1D528FFC1.png" height="497" width="918"/><br/></div><div><br/></div><div><br/></div><div><br/></div><div><img src="SL1%20-%20Decision%20Trees.html.resources/4EA6F580-980C-476C-A128-6FB42849AB2E.png" height="615" width="1095"/><br/></div><div><br/></div><div>n-xor change representation to sum of all attributes</div><div><br/></div><div>n-xor is hard, or is easy, in real, we don’t know complexity of function</div><div><br/></div><div>if we are searching all possible decision trees for the right one, how many decision trees do we actually have to look at</div><div>- n attributes - O(n!) trees, O(2**n) outputs.</div><div><br/></div><div><img src="SL1%20-%20Decision%20Trees.html.resources/191DD50E-A902-4050-90A0-2FDFC4191C70.png" height="493" width="924"/><br/></div><div><br/></div><div>2^(2^N) different trees</div><div><br/></div><div>hypothesis space of decision trees is <u>very expressive</u>, need algorithm to search these possible trees.</div><div><br/></div><div>ID3</div><div><ul><li>loop</li><ul><li>A &lt;- best attribute</li><li>assign A as a decision attribute for <u>node</u></li><li>for each value of A</li><ul><li>create a descendant of <u>node</u></li></ul><li>sort training examples to leaves</li><li>if examples perfectly classified STOP</li><li>else iterate over leaves</li></ul></ul><div><br/></div><div>picking the best attribute is about maximizing information gain or reducing entropy (impurity)</div><div><br/></div><div><img src="SL1%20-%20Decision%20Trees.html.resources/A1D1C697-EEDB-465B-862B-8B261B389E3B.png" height="235" width="367"/>   Entropy = <img src="SL1%20-%20Decision%20Trees.html.resources/4545FBC2-B384-4864-B10B-719137A70791.png" height="133" width="383"/><br/></div></div><div><br/></div><div>fair coin has 1 bit of entropy (impurity, randomness)</div><div><br/></div><div>restriction bias - hypothesis set being considered</div><div>preference bias - what sorts of hypotheses are preferred</div><div><br/></div><div>ID3 prefers trees with the biggest impact at the top (good splits at top)</div><div>ID3 prefers correct trees wrt training data</div><div>ID3 prefers shorter trees (extends from having good splits at the top and needing shorter trees)</div><div><br/></div><div>Other considerations</div><div> </div><div><ol start="1"><li><span>Continuous attributes (age, weight, distance, etc) - use thresholds</span><br/></li></ol></div><div>- we’re making discrete questions about continuous </div><div>- in this way, we could ask multiple, different questions about the same continuous attribute in the same path</div><div><br/></div><div><ol start="2"><li><span>When do we stop</span><br/></li></ol></div><div>- everything classified correctly</div><div>- out of attributes</div><div>- no overfitting (splitting produces no info gain)</div><div><span>    - post-rule pruning</span><br/></div><div><span><span>    - prune as you go</span><br/></span></div><div><span><span><br/></span></span></div><div><ol start="3"><li><span>Regression</span><br/></li></ol></div><div>- continuous - splitting, variance instead of info gain, </div><div><br/></div><div><font style="font-size: 18px;"><b>Mitchell Book Chapter 3 - Decision Trees</b></font></div><div><div/><div><div/></div><div><font style="font-size: 14px;"/><br/></div><div>Method for approximating discrete-valued target functions, inductive inference</div><div><br/></div><div>each node in the tree is a test of some feature</div><div>each branch in the tree is a possible value of the feature</div><div><br/></div><div>Decision Trees are a disjunction of conjunctions of constraints of the feature values. One path through the tree is a conjunction of tests with specific feature values. The tree is a disjunction of those paths.</div><div><br/></div><div>Appropriate DTL problems</div><div>- attribute, value pairs - temperature - Hot, Mild, Cold or 58F</div><div>- target function has discrete output values - Boolean, Actin</div><div>- Disjunctive description may be required</div><div>- training data may contain errors - DTL robust to errors</div><div>- training data may contain missing feature values</div><div><br/></div><div>Examples - Classification</div><div>- classifying medical patients - sick/healthy, cancer/not</div><div>- equipment malfunctions</div><div>- loan applicants - loan/no loan</div><div><br/></div><div>Basic DTL algorithm - ID3 (Quinlan 1986)</div><div>- top-down greedy search</div><div><span>    - which attributes should be tested at the root of the tree - each instance attribute evaludated using statistical test to determine how well it classifies training examples</span><br/></div><div><span><span>    - which attribute is best?</span><br/></span></div><div><span><span><span><span><span>    <span>    - entropy - characterization of the purity of an arbitrary collection of samples, measure of expected encoding length in bits</span></span><br/></span></span></span></span></div><div><div/><div><span><span><span><span><span>    <span>    <span>    - 1 = even distribution (impure)</span></span></span><br/></span></span></span></span></div><div><span><span><span><span><span><span><span><span>    <span>    <span>    - 0 = all samples in the same class (pure)</span></span></span><br/></span></span></span></span></span></span></span></div><div><span>    <span>    <span>    - Boolean Entropy(S) = -p1*log2p1 - p2*log2p2, where p1 is prop of samples that are p1, p2 is prop of samples that are p2</span></span></span><br/></div></div><div><span>    <span>    <span>    - Real Entropy(S) = SUM(-pi*log2pi)</span></span></span><span><span><br/></span></span></div><div><span><span>        - information gain - measure how well a given attribute <u>separates</u> the training examples according to their target classification, expected reduction in entropy from dividing set based on this attribute</span></span></div><div><span><span><span>    <span>    <span>    Gain(S, A) = Entropy(S) - SUM( (ABS(Sv)/ABS(S)) * Entropy(Sv))</span></span></span><br/></span></span></div><div><span><span><span><span><span><span>    <span>    <span>    - S = Samples</span></span></span><br/></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span>    <span>    <span>    - A = attribute</span></span></span><br/></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    <span>    - Sv = subset of S with value V of attribute A</span></span></span><br/></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    <span>    the gain is the entropy of S minus the expected entropy if A were the next attribute split</span></span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>- Hypothesis Space Search</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - ID3 is a simple-to-complex, hill-climbing search, which means</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    - (good) ID3’s hyptothesis space is complete, i.e., it will arrive at a target function</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    - (bad) ID3 only maintains single current hypothesis</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    - (bad) No backtracking - could reach locally optimal, not globally optimal solution</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    - (good) Uses all training examples</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>- Inductive Bias - set of assumptions that, together with training data, deductively justify the classifications assigned by the learner to future data</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - ID3 favors smaller trees</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - ID3 puts attributes with higher information gain at the top</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - ID3’s bias is a <u>preference bias</u> - restriction bias is another type, which restricts hypothesis space<br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Issues in DTL</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>- avoid overfitting - hypothesis overfits if a less-well fitted hypothesis does better on future data</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - prematurely stop tree from growing or allow to complete then prune</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    - what is ideal size?</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    1 - use separate data to prune - 2/3 for training, 1/3 for validation</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    2 - used all data for training, but apply statistical test to estimate if expanding node improves performance</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    A - reduced error pruning - consider each node as candidate for pruning, removing node only if resulting pruned tree performs <u>no worse</u> than original over validation set<br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    B - rule post-pruning</span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    a - infer decision tree allowing training data to be completely fit</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    b - convert tree into set of i-then rules for each path</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    c - prune each rule if removing precondition improves accuracy</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>    <span>    d - sort rules by estimated accuracy, use them in order on new data</span></span><br/></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div><div><br/></div><div>C4.5 evaluations performance based on training set, using pessimistic estimation to make up for the fact that training set gives an estimated bias by rules</div><div><br/></div><div>Continuous valued variables - what thresholds to use</div><div>- Temp - 40  48  60  72  80  90</div><div>- Tennis? N   N    Y   Y    Y   N</div><div>max info gain at 54, 85 (between changes, so rules would be temp &gt; 54 and temp &gt; 85</div><div><br/></div></div><div>Natural bias of info gain measure favors attribute with more distinct values than attribute with fewer samples (date/time often favored)<br/></div><div><br/></div></body></html>
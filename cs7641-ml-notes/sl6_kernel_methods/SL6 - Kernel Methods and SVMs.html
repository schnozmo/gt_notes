<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Boosting, Supervised Learning, SVMs"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-01-24 03:42:44 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-02-26 20:07:38 +0000"/><title>SL6 - Kernel Methods and SVMs</title></head><body><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/6B1B7D33-0D94-4D48-BD97-F717854C5231.png" height="354" width="696"/><br/></div><div>Middle line fits the data best while committing the least to the data. <u>Goal -&gt; be consistent to the data but commit the least to it.</u></div><div><br/></div><div>y = wTx + b - equation of a hyperplane</div><div><br/></div><div>y - label (in the class or out)</div><div>w - parameters of the plane</div><div>wT = w-transpose</div><div><br/></div><div>decision line/place as far away from data as possible while being consistent</div><div><br/></div><div>point on the line (not sure if positive or negative), wTx + b = 0</div><div><br/></div><div>labels are -1 and +1</div><div><br/></div><div>label for <u>positive</u> point closest to the decision plane = 1, line through this point parallel to decision plane is wTx + b = 1</div><div>label for <u>negative</u> point closest to the decision plane = -1, line through this point parallel to decision plane is wTx + b = -1</div><div><br/></div><div>line from wTx + b = 1 and wTx + b = -1 perpendicular to decision plane. we want this line to be maximal</div><div><br/></div><div>wTx1 + b = 1 ( point on pos line)</div><div>wTx2 + b = -1 (point on neg line perpendicular to pos line)</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/332A5037-22C4-4F66-9C9C-A566FDB85E38.png" height="338" width="888"/><br/></div><div><br/></div><div><br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/34F41E4A-CA78-404C-BB78-FC19217BD072.png" height="388" width="761"/><br/></div><div><br/></div><div>line between pos plane and neg plane =&gt; margin, want to max margin length (2/len(w))</div><div><br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/5DAC12FB-1E81-458B-9A8B-B88643DCBCF3.png" height="181" width="730"/><br/></div><div/><div>wTxi + b -&gt; linear classifier (hypothesis?)</div><div>yi = label</div><div>maximize 2/len(w) while classifying everything correctly (re wTxi + b agrees with yi, (either both 1 or -1)</div><div><br/></div><div>OR</div><div><br/></div><div>minimize 1/2 * len(w)**2 - </div><div><br/></div><div>len(w) —  2/len(w) — 1/2*len(w)**2</div><div>1 ——   2 — —— 0.5</div><div>4 ——    0.5 ——— 8</div><div>10  —— 0.2 ——— 50</div><div>25 —— 0.08 ——— 312.5</div><div><br/></div><div>Does this really just mean minimize the features that still answer the question?</div><div><br/></div><div>min( 1/2 * len(w)**2 ) is a quadratic programming problem and easier to solve, which <u>a<span>lways has a unique solution</span></u></div><div><br/></div><div><br/></div><div>maximize sum of all i (data points) of alphas - for every pair of examples, product of alphas, labels, and values</div><div>such that all alphas &gt;= 0 and sum of product of alphas and labels = 0</div><div><br/></div><div>this is equivalent to minimize 1/2 * len(w)**2</div><div><br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/CAC34A47-9075-4875-BB98-DA60FFFD0922.gif" height="59" width="330"/> <span style="line-height: 1.45;">such that  </span><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/7A9C82D4-E3AC-4D5B-B251-CDCD30D71651.gif" height="48" width="180"/><br/></div><div><br/></div><div><br/></div><div>once alphas are maximized in about equation, “easy” to find w, then b</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/FF81F10C-5631-4AE8-AB71-516B9A8A6730.png" height="101" width="464"/><br/></div><div>alphas are mostly 0, corresponding data points won’t matter, non-zero alphas associated with w’s are the <u>Support Vectors</u></div><div><u/></div><div/><div>Points far from decision boundary don’t matter, alphas will be 0</div><div>- like kNN, closest points matter</div><div>- don’t need to keep all points, just those that are close to decision boundary</div><div><br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/9D420B0E-8E3C-41F0-8118-0EF0488FCE04.png" height="55" width="73"/>- only place in equation using X’s. This is a dot product, which produces the length of the projection and describes “similarity” of xiTxj. WIll be big if pointing in the same direction<br/></div><div><br/></div><div>Point <u>transformation</u> for points like these:</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/AE4CCF46-3DF1-44F5-A796-4C4183062BAC.png" height="197" width="211"/><br/></div><div><br/></div><div><br/></div><div>q is in the plane as these points and has 2 components, q1 and q2, produce from these components a triple so that q is now in 3 dimensions:</div><div><br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/A6E0DA46-149D-48D2-8553-8312C9EA7231.gif" height="40" width="216"/><br/></div><div/><div>what matters most in solving this quadratic problem, and ultimately solving our optimization problem, is being able to constantly do these transpose operations. If there are 2 2-dim pts x and y, their “similarity” is measured by the dot product:<br/></div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/528F40CB-C149-4C02-BF39-A4E365AAFCA5.gif" height="24" width="35"/><br/></div><div>Instead of this, we’ll transform by passing x and y through this Phi function</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/9C86230D-F0B7-451C-BBCD-64488AC9E305.gif" height="26" width="97"/><br/></div><div>which is </div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/075B21C5-38B7-42FB-84AA-2695754AE701.gif" height="27" width="302"/><br/></div><div>then</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/CD5D1CC0-84B5-45D4-A81C-AAF327DD3B74.gif" height="26" width="345"/><br/></div><div>then</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/AB63C4E3-D5DA-4552-A6F1-D62401E13668.gif" height="26" width="127"/><br/></div><div><br/></div><div>OR</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/AE78A129-830A-47B9-BD65-BB4CF9391E5A.png" height="107" width="194"/><br/></div><div>This is like the equation for a circle. We could use this to create a circular-separable decision plane?</div><div>In this formulation of the quadratic program, every time we want to compute xiTxj, if we squared it beforehand, we can project it into another dimension and find a plane =&gt; <u>kernel trick.</u> Notion of similarity conditional on representing as a dot product.</div><div><br/></div><div><u>The kernel is the function that we use to project xiTxj </u></div><div><br/></div><div>weights equation can now be rewritten as:</div><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/E0C3E615-47D4-4474-ADE6-7CF3E3122718.png" height="138" width="662"/><br/></div><div>where k(xi, xj) is the kernel function. This is still a measure of similarity and captures domain knowledge</div><div>some kernels:</div><div>k = (xTy)_2</div><div>k = xTy</div><div>k = (xTy + c)**p</div><div>k = e ** - ( ||x - y||**2 / 2 * omega ** 2)   —&gt; this is a radial basis function</div><div>k = tanh (alpha * xTy + theta)  —&gt; sigmoid-like function</div><div><br/></div><div>are their bad kernel functions? there is a mathematical requirement (Mercer Condition) for a kernel function</div><div><br/></div><div>Summary:</div><div>- margin is a way to measure linear separation and generalization, bigger is better</div><div>- opt problem for maximizing margins - Quadratic programs</div><div>- support vectors (vectors with non-zero weights)</div><div>- can project data into a higher-dimensional space using a kernel function, xTy ==&gt; K(x, y)</div><div><br/></div><div>Back to Boosting:</div><div><br/></div><div>boosting doesn’t always overfit (at least in ways we normal expect)</div><div>- testing error decreases over time just as training error</div><div>- <span>confidence - how much do we believe the hypothesis, variance</span></div><div>- H-final(x) = sign(  sum/t(  alpha_t * h_t(x)  )</div><div>- if we divide this by the sum of the alphas, this normalizes the result to a range of -1..1</div><div>- individual examples then can be placed along a continuum in this range, the closer each individual is to a pole, the more confident we are in the result</div><div><br/></div><div/><div><img src="SL6%20-%20Kernel%20Methods%20and%20SVMs.html.resources/0740453D-20A0-44C8-A5EA-817BB784EA8A.png" height="469" width="532"/><br/></div><div>over t as we add weak learners, the error remains the same, but the pluses and minuses move closer to the poles and create bigger distances between examples. This is similar to the SVM concept of margin, where the bigger margins represent H’s that are not likely to overfit</div><div><br/></div><div>Boosting could overfit if</div><div>- weak learner overfits</div><div>- pink noise - uniform noise</div><div><br/></div></body></html>
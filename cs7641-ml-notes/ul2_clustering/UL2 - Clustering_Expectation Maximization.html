<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Clustering, Expectation Maximization, Unsupervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-03-02 06:41:17 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-03-19 15:06:28 +0000"/><title>UL2 - Clustering/Expectation Maximization</title></head><body><div>Basic Clustering Problem:</div><div><br/></div><div>- given: set of objects (X) and inter-object distances D(x,y) = D(y,x) x,y member of X</div><div>    - distance represents similarity between objects, this is the domain knowledge</div><div><br/></div><div>- output: Partition <img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/2EDB1EA8-F2D9-4DEC-ADF6-BC999B45AB08.gif" height="18" width="117"/>if x and y in the same cluster (same label)</div><div><br/></div><div>Simple partitions:</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/C54B12ED-4E29-4444-9483-690A03671FA7.gif" height="18" width="100"/> —&gt; all points in the same cluster</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/78AC444D-1A36-4154-8AD1-A50324929AA2.gif" height="18" width="102"/>—&gt; each point in its own cluster</div><div><br/></div><div><b>Single Linkage Clustering (SLC simplest, most natural)</b></div><div>- consider each object a cluster (n objects)</div><div>- define intercluster distance as the distance between the closest two points in the two clusters</div><div>    - starts with all points in their <u>own</u> cluster</div><div>- merge 2 closest clusters by closest points, not average of points in cluster</div><div>- repeat n-k times to make n clusters to leave with k clusters</div><div><br/></div><div>SLC is a Hierarchical Aglomerative Clusting algorithm (HAC)</div><div><br/></div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/520800AB-1689-47EE-AC1B-21240EE82CF2.png" height="363" width="376"/><br/></div><div>This forms a sort of tree</div><div>Algorithm input (besides n)</div><div>k - number of clusters to end up with</div><div>Defining inter-cluster distance (closest point, mean distance) - another instance of domain knowledge and algorithm input</div><div><br/></div><div>further - metaphorical</div><div>farther - physical</div><div><br/></div><div>Running time - O(n**3)</div><div>- to find 1st 2 points to merge requires n**2 time</div><div>- worst case is 1 cluster, so repeat n times = n**2 * n = n**3</div><div><br/></div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/D16CB4DC-D5DF-41E4-A306-E9DE2B186122.png" height="478" width="843"/><br/></div><div><br/></div><div><b>K-Means Clustering</b></div><div><b><br/></b></div><div>- pick k centers (at random)</div><div>- repeat until convergence</div><div>    - each center “claims” its closest points</div><div>    - recompute the centers by averaging the clustered points (<u>not necessarily one of the points</u>!)</div><div>Step 1 - <img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/EF6CF4E5-7CAD-431F-8A7C-CE31CC401944.png" height="255" width="411"/><br/></div><div>Step 2 - <img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/1D9E60A5-C96C-41D0-982D-1E642EC0185F.png" height="274" width="402"/><br/></div><div>Step 3 - <img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/CC2327A1-B161-4DE7-A8FA-6252C3B080C9.png" height="240" width="395"/><br/></div><div>Convergence !!!</div><div><br/></div><div>Does this always converge?</div><div>Does it produce something useful?</div><div><br/></div><div>K-means in Euclidean space</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/7802BA80-8774-4C05-BBB7-D91F929B0F14.gif" height="20" width="43"/>: partition or cluster of object x at time t</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/7662E728-8DDA-4058-9AA8-17496AD9ED6C.gif" height="21" width="19"/>: set of all points in the cluster i = {x s.t. P(x) = i} at time t</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/F103752B-2C02-4FA4-AA45-23E859C11323.gif" height="44" width="188"/>: center of cluster (centroid) - this is just the mean at time t of all points in cluster</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/4D52113E-FBD5-4DF4-BEB1-F3466F82563C.png" height="275" width="966"/><br/></div><div>the “argmin” part assigns each point to it’s closest center. Then, these partitions are passed to the centering function which finds the new center for each partition. These centers are now passed back so that all points pick a new partition with a closest center.</div><div><br/></div><div>This is a kind of an optimization algorithm</div><div>- configurations - center, partition</div><div>- scores - error is the mean dist from center for all points<img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/42B2EDBA-904D-4FED-9752-8D99652E56EA.gif" height="40" width="326"/><br/></div><div><br/></div><div>- neighborhood - one way pairs where we would change either the partition or the center</div><div><br/></div><div>Similar to hill-climbing, since centers are taking iterative steps to reduce the error (kind of depends on random centers first chosen). Error can only decrease or stay the same, since we only ever re-assign points to partitions if the distance the the partition center is less than before. The centers can’t move farther from the points in the partition, since they are just averages that minimize squared error.</div><div><br/></div><div>Ties in distance from center must be resolved consistently, e.g., go to the lower P ID</div><div><br/></div><div>configurations will never repeat</div><div>each iteration is polynomial O(k * n) - find k centers, assign n points to centers </div><div>k**n possible configurations/iterations (finite)</div><div>error decreases or stays the same</div><div><br/></div><div>avoid local optima</div><div>- random restarts</div><div>- some initial selection based on analysis of the points</div><div><br/></div><div><b>Soft Clustering</b></div><div><b><br/></b></div><div>Points aren’t statically assigned to clusters, but we can tell probablistically how likely a point is to belong to a cluster.</div><div><br/></div><div>Assume data generated by</div><div>1 - select one of K Gaussian with fixed, known variance uniformly</div><div>2 - sample x_i from that Gaussian</div><div>3 - repeat n times</div><div><br/></div><div>Task: find a hypothesis h = &lt;mean_1 … mean_h&gt; that maximizes the probability of the data (maximum likelihood).</div><div><br/></div><div>The ML mean of the Gaussian is the mean of the data (remember a Gaussian is a mean and a variance). If there are K means, need to set the k different means using <u>hidden variables</u>. The data points will be the point x, then a bunch of indicator variables z_1…z_k</div><div><br/></div><div><br/></div><div><b>Expectation Maximization</b></div><div><b><br/></b></div><div>cycle between a soft clustering assignment (expectation) and center setting (maximization). E[Z_ij] is the likelihood that data element i comes from cluster j. Likelihood implies Bayes rule without a prior probability</div><div><br/></div><div>Expectation (derive Z from mu) —&gt;<img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/9421BA9F-E25C-4C90-ABAE-A458308BF03F.gif" height="50" width="248"/><br/></div><div><br/></div><div>Maximization (derive mu from Z) —&gt; <img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/14ABD949-3639-45C1-89C0-289319CF7AD3.gif" height="45" width="135"/><br/></div><div><br/></div><div>Expectation feeds Z’s to Maximization, which returns mu’s </div><div><br/></div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/4AF9DEBF-CBCE-4227-9446-F6CA3A74C205.png" height="24" width="259"/><br/></div><div><br/></div><div>Data set with X the first randomly chosen mu's</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/77AF6015-786D-4BC9-A48E-1FE9C57E65F1.png" height="493" width="545"/><br/></div><div><br/></div><div>Expectation/Maximization 1 - the first pass assigns points close to one X blue, the other X red, and some with p &gt; 0 for both points in green. The mu’s (x points) are then updated in the Maximization step based on the “average” point in the cluster.</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/0B579071-DF5B-4835-87AC-47C32E032EDD.png" height="483" width="541"/><br/></div><div><br/></div><div>Expectation/Maximization 2 - </div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/88A93194-086E-4D7D-8B80-0BEAE53757C6.png" height="486" width="543"/><br/></div><div>Expectation/Maximization 5 - </div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/26C140FC-F128-4A54-AE18-AB1C102F059E.png" height="487" width="547"/><br/></div><div><br/></div><div>EM not forced to decide a cluster for every point. Even points that are visually close to the cluster center have p&gt;0 for the other cluster center</div><div><br/></div><div>EM can be applied in lots of settings, not just Gaussians</div><div><br/></div><div>Properties of EM</div><div>- monotonically non-decreasing likelihood (like k-means)</div><div>- does not converge (practically does) - infinite number of probabilities/configurations, but individual steps can get very, very small</div><div>- will not diverge (diff from k-means)</div><div>- can get stuck in local optima - random restart</div><div>- works with any distribution (if E,M solvable)</div><div>- estimation is expensive stem, usually</div><div><br/></div><div>Clustering Properties (P_D) is the clustering scheme (partitions given a distance matrix)</div><div>- Richness - for any assignment of objects to clusters, there is some distance matrix D such that P_D returns that clustering (there can be one cluster or two or 3…)</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/6D6A9671-80FB-4B0E-8DE3-87574A64AE17.png" height="17" width="151"/><br/></div><div>- Scale-invariance - scaling distances by a positive value doesn’t change the clustering</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/A3680993-48CC-482A-8977-1D4ABEA0AC5E.png" height="17" width="172"/><br/></div><div>- Consistency - shrinking intra-cluster distances and expanding inter-cluster distances doesn’t change the clustering</div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/AFC64429-ACFC-438B-A07D-DDC8694E553E.png" height="15" width="73"/><br/></div><div>distances are a measure of similarity</div><div><br/></div><div><br/></div><div><img src="UL2%20-%20Clustering_Expectation%20Maximization.html.resources/8010C804-E6CF-4D80-AD29-4639C2FAEF3F.png" height="426" width="853"/><br/></div><div>- the n/2 clusters algorithm is not rich, because I am forced to go to n/2 clusters</div><div>- the theta units apart algorithm isn’t scale invariant because I’ll get a different answer if I multiply the distances by a constant</div><div>- the theta/omega apart algorithm isn’t consistent because expanding inter-cluster distances will stretch omega and make theta/omega smaller</div><div><br/></div><div>No clustering scheme can achieve all three (Kleinberg) - they are mutually contradictory.</div><div><br/></div><div>Good for getting to know data, not great for automating</div><div><br/></div><div>Summary</div><div>- clustering: the idea</div><div>- connection to compact description</div><div>- EM makes for soft cluster vs hard clustering (SLC, k-means)</div><div>- SLC is fast</div><div><br/></div></body></html>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="keywords" content="Bayes, Supervised Learning"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-02-12 22:47:09 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-02-25 04:09:57 +0000"/><title>SL10 - Bayesian Inference</title></head><body><div>Bayesian networks</div><div><br/></div><div>Joint Distribution</div><div><br/></div><div>Storm   Lightning   - what is the probability of combination of both events</div><div>T            T                   .25</div><div>T            F                   .4</div><div>F            T                   .05</div><div>F            F                   .3</div><div><br/></div><div>P( ! storm ) = 0.35</div><div>P( lightning | storm ) = .25 / (.25 + .4) = .38</div><div><br/></div><div><br/></div><div>Storm   Lightning   Thunder</div><div>T            T               T .2,    F .5</div><div>T            F               T .04   F .36</div><div>F            T               T .04   F .01</div><div>F            F               T .03   F .27</div><div><br/></div><div>Factor into smaller pieces we can recombine</div><div><br/></div><div>Conditional Independence </div><div>DEFN: X is conditionally independent of Y given Z if the probability distribution governing X is independent of the value of Y given the value of Z</div><div><br/></div><div>IOW, for all x, y, z, P( X = x | Y = y and Z = z ) = P( X = x | Z = z )</div><div>IOW, for all x, y, z, P( X | Y and Z ) = P( X | Z )</div><div><br/></div><div>Normal Independence</div><div>P( X and Y ) = P( X ) * P( Y )</div><div><br/></div><div>Chain rule</div><div>P( X and Y ) = P( X | Y ) * P( Y )</div><div><br/></div><div>Marginalization rule</div><div>P( X ) = P( X, !Y ) + P( X, Y )</div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/26C7BD77-AC37-4675-BAE9-CEE1C9DE7870.png" height="464" width="892"/><br/></div><div>So, with thunder = T and lightning = F, when storm is T, p = .04/.4 = .1</div><div>                                                              when storm is F, p = .03/.3 = .1</div><div><br/></div><div>In fact, no matter what truth pairs used for thunder and lightning, p’s will be the same, thus, thunder and storm are conditionally independent given lightning.</div><div><br/></div><div>Belief Networks (aka Bayes Nets aka Graphical Models)</div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/A5B60EFD-5760-4266-A872-A1DE5DEB8C3E.png" height="413" width="866"/><br/></div><div>P( S ) = 0.65</div><div><br/></div><div>P( L | S ) = .25/( .25 + .4 ) = 0.38</div><div>P( L | !S ) = .05/( .05 + .3 ) = 0.14</div><div><br/></div><div>P( Th | L ) = 0.2 / 0.25 = 0.8   ### Since conditionally independent of storm, only one set needed - </div><div>P( Th | !L ) = 0.04 / 0.4 = 0.1  ###  it doesn’t matter what storm is</div><div><br/></div><div>belief network will grow exp with more variables.</div><div>belief networks are not dependents</div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/72F108D0-FBDA-41B6-895C-679AFF5F96EE.png" height="460" width="842"/><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/830C289E-0F76-47F6-AE24-F3B2F14BFB7C.png" height="218" width="231"/><br/></div><div><br/></div><div>Joint probability of some assignment of variables = product of the individual variables conditioned on their inputs</div><div><br/></div><div>Probability of A or B (below), could have 1 value, since they are independent from other variables</div><div>Probability of C, D, or E could have 4 values, because each is dependent on 2 other variables, which have 4 combinations of values</div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/91EB3C5F-7144-4E8C-96F3-54204970E07E.png" height="541" width="996"/><br/></div><div><br/></div><div>Why a distribution?</div><div>- given a value, what’s the probability of that value</div><div>- generate a value from that distribution</div><div>- if a distribution represents a process, we can simulate that process by drawing from that distribution</div><div>- approximate inference - what might be true given certain situations (machine)</div><div>- visualization - get a feel of the data (human)</div><div><br/></div><div>Bayesian Inference by hand</div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/C57657CF-165F-4C32-9198-2EAC5A6FC839.png" height="197" width="357"/><br/></div><div>Pick 1 green ball from one of the boxes, then pick a second ball from the same box. What is the probability that the 2nd ball is blue, given the first ball was green? P( 2 = blue | 1 = green )</div><div><br/></div><div>Box —&gt; Ball1 —&gt; Ball2    Ball2 depends on value of ball 1 and the box that ball 1 is in</div><div>     \———————/</div><div><br/></div><div>              G      Y      B                                                            G      Y      B</div><div>Box 1    3/4   1/4    0            After picking ball 1    Box 1     2/3   1/3     0</div><div>Box 2    2/5    0      3/5                                           Box 2    1/4     0      3/4</div><div><br/></div><div><br/></div><div>Marginalization rule + Chain rule</div><div>P( 2 = blue | 1 = green) = <font color="#9d3992">P( 2 = blue | 1 = green, Box = 1)</font> *<font color="#537a8d"> P( box = 1 | 1 = green) </font>+</div><div>                                        <font color="#e04520">P( 2 = blue | 1 = green, Box = 2) </font>* <font color="#a97724">P( box = 2 | 1 = green)</font></div><div><br/></div><div><font style="color: rgb(157, 57, 146);">P( 2 = blue | 1 = green, Box = 1) is 0, since there are no blue in box 1</font></div><div><font style="color: rgb(224, 69, 32);">P( 2 = blue | 1 = green, Box = 2) is 3/4, if we know the box is 2 and the ball is green, 3 of the 4 remaining balls are blue</font></div><div><font style="color: rgb(224, 69, 32);"><br/></font></div><div>apply bayes for other part</div><div><font color="#52798d">P( box = 1 | 1 = green )</font> = P( 1 = green | box = 1 ) * P( box = 1 ) / P( 1 = green ) = 3/4 * 1/2 / P( 1 = green ) = 3/8 / P( 1 = green )</div><div><font color="#a97724">P( box = 2 | 1 = green )</font> = P( 1 = green | box = 2 ) * P( box = 2 ) / P( 1 = green ) = 2/5 * 1/2 / P( 1 = green ) = 1/5 / P( 1 = green )</div><div><br/></div><div>3/8 + 1/5 = 15/40 + 8/40 = 23/40, so P( box = 1 | 1 = green ) = 15/23 and P( box = 2 | 1 = green ) = 8/23. If Box = 1, there is 0 probability that ball2 = blue. if box = 2, there is 3/4 probability that ball2 = blue, so 8/23 * 3/4</div><div><br/></div><div><br/></div><div>Naive Bayes: Special Case</div><div><br/></div><div>Spam example - if we know that it is spam, generate the set of words in the message</div><div><br/></div><div>Given 3 words, “Viagra”, “Prince”, “Udacity”, their appearance in email is dependent on whether the message is spam</div><div><br/></div><div>P( “Viagra” | Spam ) = 0.3, P( “Viagra” | ! Spam ) = 0.001</div><div>P( “Prince” | Spam ) = 0.2, P( “Prince” | ! Spam ) = 0.1</div><div>P( “Udacity” | Spam ) = 0.0001, P( “Udacity” | ! Spam ) = 0.1</div><div><br/></div><div>P( Spam ) = 0.4, P( ! Spam ) = 0.6</div><div><br/></div><div>What is P( Spam | Viagara, ! Prince, ! Udacity )? To answer, we can flip it around to:</div><div>P( Viagara, ! Prince, ! Udacity | Spam ) * P( Spam ) = </div><div>P( Viagra | Spam ) * P( ! Prince | Spam ) * P( ! Udacity | Spam ) * P( Spam) = </div><div>.3 * .8 * .9999 * .4 = 0.09</div><div><br/></div><div><br/></div><div>P( root value | attributes ) = Product_i ( P( a_i | root value ) * P( root value) ) / Z</div><div><br/></div><div>MAP class = argmax_v (Product_i (P( a_i | V ) )</div><div><br/></div><div>remember, MAP = maximum a posteriori</div><div><br/></div><div>Why inference is cheap?</div><div>- inference is cheap</div><div>- linear with parameters</div><div>- estimate parameters with labeled data  - P( a_i | class ) = count(a_i, class) / count( class )</div><div>- connects inference with classification - can go attribute -&gt; value or value -&gt; attribute</div><div>- empirically successful</div><div><br/></div><div>Naive - painful that simple bayesian network represents the world. doesn’t model interrelationships between attributes. every attribute has the same parent</div><div><br/></div><div>For classification, the probabilities don’t matter since the ordering is preserved, only the MAP class</div><div><br/></div><div>what is one attribute has not been seen in a class before? count( a_i, class) / count( class ) = 0</div><div>this is an inductive bias and leads to overfitting</div><div><br/></div><div><br/></div><div><img src="SL10%20-%20Bayesian%20Inference.html.resources/6071CE5C-CB14-425F-9A0F-CDD14413DDBC.png" height="568" width="847"/><br/></div><div><br/></div></body></html>
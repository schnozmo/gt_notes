<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.8 (457453)"/><meta name="author" content="jmichaelbrunner@gmail.com"/><meta name="created" content="2017-09-12 03:23:04 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-11-12 22:34:13 +0000"/><title>Mini Course 3 Lectures</title></head><body><div><span style="font-size: 18px;">Machine Learning Algorithms for Trading</span></div><div><br/></div><div><span style="font-size: 18px; line-height: 26px;">SMA = simple moving average</span></div><div><br/></div><div><h1 style="padding: 0px; border: 0px; font-size: 24px; vertical-align: baseline; text-transform: capitalize; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-style: normal; font-weight: 600;">03-01 How Machine Learning is used At a Hedge Fund</span></h1><div><br/></div></div><div>Supervised Regression Learning - numerical prediction</div><div><br/></div><div>- linear regression (parametric) - create model from data</div><div>- knn (instance-based) - historic data used in prediction</div><div>- decision trees</div><div>- decision forests</div><div><br/></div><div>Problems with regression</div><div>- noisy and uncertain</div><div>- challenging to estimate confidance</div><div>- holding time, allocation (when to act) - could use policy learning (RL)</div><div><br/></div><div><br/></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: 600;">03-02 Regression</span></div><div><br/></div><div>build a model that produces a numeric output given a set of numeric inputs</div><div><br/></div><div>KNN - k Nearest Neighbors - mean of nearest k points to input</div><div>kernel regression - weighted mean of nearest points</div><div><br/></div><div>connecting test points if using knn looks like this:</div><div><img src="Mini%20Course%203%20Lectures.html.resources/3C07E9D8-C123-4649-928C-A42994757B02.png" height="550" width="919"/><br/></div><div><br/></div><div>Parametric - training slow, testing fast, space efficient, periodic recompute model, increasing the polynomial order tends to overfit</div><div>Instance - training 0 time, testing slow, lots of storage space required, increasing k avoids overfitting</div><div><br/></div><div>Out of sample testing - testing on different data than trained on</div><div><br/></div><div>For stock data, train on older data, test on newer data</div><div><br/></div><div>Learning APIs for this class</div><div>- learner = LinRegLearner()</div><div>- learner.train(Xtrain, Ytrain)</div><div>- y = learner.query(Xtest)</div><div><br/></div><div>- learner = KNNLearner(k = 3)</div><div>- learner.train(Xtrain, Ytrain)</div><div>- y = learner.query(Xtest)</div><div><br/></div><div>Example</div><div><br/></div><div><span style="font-family: 'Courier New';">class LinRegLearner::</span></div><div><span style="font-family: 'Courier New';">    def __init__():</span></div><div><span style="font-family: 'Courier New';">        pass</span></div><div><span style="font-family: 'Courier New';">    def train(x,y):</span></div><div><span style="font-family: 'Courier New';">        self.m, self.b = favorite_linreg(x, y)</span></div><div><span style="font-family: 'Courier New';">    def query(x):</span></div><div><span style="font-family: 'Courier New';">        y = self.m * x + self.b</span></div><div><span style="font-family: 'Courier New';">    </span><span style="font-family: 'Courier New';">    return y</span></div><div><br/></div><div><br/></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: 600;">03-03 Assessing a learning algorithm</span></div><div><br/></div><div>RMS error - root mean squared error</div><div><br/></div><div>sqrt ( sum( ( y_test - y_predict ) ** 2 ) / N )</div><div><br/></div><div>in of sample error - training error</div><div>out of sample erro - test/cv error</div><div><br/></div><div>traditional cross validation doesn’t work well with financial data - hard to train on future folds to predict past folds</div><div>roll-forward cross-validation - training fold always before test fold</div><div><br/></div><div>Compare Ytest (y axis) with Ypredict (x axis)</div><div><br/></div><div>np.corrcoef() -&gt; -1 = inversely correlated, 1 = correlated, 0 = no correlation</div><div><br/></div><div>RMS inc, correlation dec</div><div><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/AD841327-DD70-49D0-8C6B-33C5213EBC1C.png" height="549" width="807"/>   <img src="Mini%20Course%203%20Lectures.html.resources/20D03E03-9BAF-4DF9-8D9D-FFAA98F915D1.png" height="318" width="756"/><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: 600;">03-04 Ensemble learners, bagging and boosting</span></div><div><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/4E081665-3A2B-4DF9-8BC7-ABD56D0CB677.png" height="653" width="1096"/><br/></div><div><br/></div><div>less overfitting because overcome individual learner’s bias</div><div>- linreg - order polynomial</div><div>- decision tree - focus on highest info gain first</div><div><br/></div><div><br/></div><div>bagging (bootstrap aggregation) - randomly sample (with replacement so could get duplicates) n’ instances of n total data points in m buckets. Train m models. For test/lookup, run x through all m models and take mean/weighted vote</div><div><br/></div><div>boosting - first bag is random sample, 2nd bag contains error data points</div><div><br/></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: bold;">03-05 - Reinforcement learning</span></div><div><br/></div><div>at state s, pi is the policy to follow so that an action a is taken to a new state s’ is reached maximizing the reward r</div><div><br/></div><div>Actions - buy, sell</div><div>States - holding long, bollinger value, daily return</div><div>Reward - return from trade, daily return</div><div><br/></div><div>RL solves Markov decision problems</div><div>Transition - T(s, a, s’)</div><div>Reward = R(s, a)</div><div><br/></div><div>Given T and R, we could use policy iteration and value iteration to find pi*. <span style="line-height: 1.45;">We have unknown transitions and rewards, so </span></div><div><br/></div><div><span style="line-height: 1.45;">experience tuples - trail of paths</span></div><div><span style="line-height: 1.45;">&lt;s1, a1, s</span><span style="line-height: 1.45;">1', r1&gt;,</span></div><div><span style="line-height: 1.45;">&lt;s2, a2, s2</span>’<span style="line-height: 1.45;">, r2&gt;</span></div><div><br/></div><div><span style="line-height: 1.45;">Model based - build model of T[s, a, s</span>’<span style="line-height: 1.45;">] and R[s, a] - use value/policy iteration</span></div><div><span style="line-height: 1.45;">Model-free - QLearning </span></div><div><br/></div><div>infinite horizon - sum of rewards without any maximum number of iterations</div><div>finite horizon - sum of rewards for a specified max number of steps</div><div>discounted reward = devalued future reward = sum(\gamma * r), where \gamma &lt; 1</div><div><br/></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: bold;">03-06 - Q learning</span></div><div><br/></div><div>Qlearning builds a table of utility values as it explores it’s world. Guaranteed to find an optimal policy</div><div><br/></div><div>Q[s, a] = value of taking action a in state s = immediate reward + discounted reward = </div><div>\pi*_s = argmax_a( Q[ s, a ] )</div><div><br/></div><div>iterate through different possible actions at each state</div><div><br/></div><div><ul><li><span style="line-height: 1.45;">select training data</span></li><li><span style="line-height: 1.45;">iterate over time &lt;s, a, s</span>’<span style="line-height: 1.45;">, r&gt;</span></li><ul><li><span style="line-height: 1.45;">set starttime, init Q[]</span></li><li><span style="line-height: 1.45;">compute s - features of stocks/portfolio</span></li><li><span style="line-height: 1.45;">select an action</span></li><li><span style="line-height: 1.45;">observe r, s</span>’</li><li><span style="line-height: 1.45;">update Q[]</span></li></ul><li><span style="line-height: 1.45;">test policy pi</span></li><li><span style="line-height: 1.45;">repeat until converge</span></li></ul></div><div><br/></div><div>update rule</div><div>- alpha is learning rate 0-1, usually 0.2 for this class</div><div>- gamma is discount rate (how high do we value later returns</div><div><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/6FEBC6ED-BC66-43E8-9354-902085E7D2EC.png" height="18" width="399"/><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/4FA6EDB3-EA91-401B-8EA3-1EA65F2E5C5B.png" height="18" width="418"/><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/CEA30BDD-278C-40B3-8D08-19115AAAE3E6.png" height="19" width="496"/><br/></div><div style="padding: 0px; border: 0px; font-size: 1rem; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><div><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;">The formula for computing </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">Q</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> for any state-action pair </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">&lt;s, a&gt;</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;">, given an experience tuple </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">&lt;s, a, s', r&gt;</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;">, is:</span></div><div style="-en-paragraph: true; margin-top: 1em; margin-bottom: 1em;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmax</span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; vertical-align: sub; font-size: smaller; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; line-height: 1.7;">a'</span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">(Q[s', a'])])</span></div></div><div style="padding: 0px; border: 0px; font-size: 1rem; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><span style="border: 0px; font-size: 1rem; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 1.7;">Here:</span></div><ul style="padding: 0px 0px 0px 40px; border: 0px; font-size: 15px; list-style: disc; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style="padding: 0px; border: 0px; font-size: 1rem; vertical-align: baseline; list-style: disc;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">r = R[s, a]</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> is the immediate reward for taking action </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">a</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> in state </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">s</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;">,</span></li><li style="padding: 0px; border: 0px; font-size: 1rem; vertical-align: baseline; list-style: disc;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">γ ∈ [0, 1]</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> (gamma) is the </span><span style="border: 0px; background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7; font-style: italic;">discount factor</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> used to progressively reduce the value of future rewards,</span></li><li style="padding: 0px; border: 0px; font-size: 1rem; vertical-align: baseline; list-style: disc;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">s'</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> is the resulting next state,</span></li><li style="padding: 0px; border: 0px; font-size: 1rem; vertical-align: baseline; list-style: disc;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">argmax</span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; vertical-align: sub; font-size: smaller; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; line-height: 1.7;">a'</span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">(Q[s', a'])</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> is the action that maximizes the Q-value among all possible actions </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">a'</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> from </span><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">s'</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;">, and,</span></li><li style="padding: 0px; border: 0px; font-size: 1rem; vertical-align: baseline; list-style: disc;"><span style="border: 0px; background-color: rgb(249, 242, 244); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; color: rgb(199, 37, 78); font-family: 'Lucida Console', Monaco, Courier, monospace; font-size: 1rem; line-height: 1.7;">α ∈ [0, 1]</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> (alpha) is the </span><span style="border: 0px; background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7; font-style: italic;">learning rate</span><span style="background-color: rgb(255, 255, 255); color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; font-size: 1rem; line-height: 1.7;"> used to vary the weight given to new experiences compared with past Q-values.</span></li></ul><div style="orphans: 2; widows: 2;"><div><br/></div></div><div style="orphans: 2; widows: 2;"><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">success depends on exploration, relies on randomness (choose random actions in the beginning with probability c, starting 0.3</span></div><div style="orphans: 2; widows: 2;"><div><br/></div></div><div style="orphans: 2; widows: 2;"><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">Actions - buy, sell, do nothing</span></div><div style="orphans: 2; widows: 2;"><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">Rewards - daily return</span></div><div style="orphans: 2; widows: 2;"><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">State</span></div><div style="orphans: 2; widows: 2;"><ul><li><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">Price / SMA, BB value, Price / Earnings, returns since entry, are we holding stock binary</span></li><li><span style="color: rgb(79, 79, 79); font-family: 'Open Sans', Helvetica, sans-serif; line-height: 23px;">take each indicator and discretize</span></li></ul></div><div style="orphans: 2; widows: 2;"><div><br/></div></div><div style="orphans: 2; widows: 2;"><div><br/></div></div><div><span style="font-size: 24px; color: rgb(46, 61, 73); font-family: 'Open Sans', Helvetica, sans-serif; font-weight: bold;">03-07 - Dyna</span></div><div><br/></div><div>T is the probability that taking <span style="text-decoration: underline;">a</span> at <span style="text-decoration: underline;">s</span> leads to s’</div><div>hallucinate</div><div>- random s, a</div><div>- infer s’ from T</div><div>  - start with tcount[] = 0.00001</div><div>  - observe s’ with random s, a, increment tcount[s, a, s’]</div><div>Learn R</div><div>- R[s, a] = expected reward</div><div>- r = immediate reward</div><div><img src="Mini%20Course%203%20Lectures.html.resources/90607DEC-7210-4246-9D49-D38F9147D499.png" height="19" width="226"/></div><div><br/></div><div><br/></div><div><img src="Mini%20Course%203%20Lectures.html.resources/406DC4CC-48DB-4745-B6FC-9510FC88EB13.png" height="628" width="981"/></div><div>DynaQ is useful because interactions with real world is expensive, but “hallucinations” cheap</div><div><br/></div><div><div style="padding: 0px; border: 0px; line-height: 1.33333em;"><div style="padding: 0px; border: 0px; line-height: 1.7;"><h2 style="padding: 0px; border: 0px; font-weight: bold; font-size: 20px; line-height: 1.33333em; color: rgb(46, 61, 73);"><span style="border: 0px; font-weight: bold; font-size: 20px; line-height: 1.33333em; color: rgb(46, 61, 73);">Summary</span></h2><div style="padding: 0px; border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true; margin-top: 1em; margin-bottom: 1em;"><div>The Dyna architecture consists of a combination of:</div></div><ul style="padding: 0px 0px 0px 40px; border: 0px; line-height: 1.33333em; list-style: disc;"><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">direct reinforcement learning from real experience tuples gathered by acting in an environment,</li><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">updating an internal model of the environment, and,</li><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">using the model to simulate experiences.</li></ul><div style="padding: 0px; border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true; margin-top: 1em; margin-bottom: 1em;"><img src="Mini%20Course%203%20Lectures.html.resources/69C3B0FE-3ECB-40E1-93AD-B8EA45333404.png" height="243" width="333"/></div></div></div></div><div><div style="padding: 0px 15px; border: 0px; line-height: 1.33333em; max-width: 800px;"><div style="padding: 0px; border: 0px; line-height: 1.33333em;"><div style="padding: 0px; border: 0px; line-height: 1.33333em;"><div style="padding: 0px; border: 0px; line-height: 1.7;"><div style="padding: 0px; border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true; margin-top: 1em; margin-bottom: 1em;"><span style="border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true;">Sutton and Barto. </span><span style="border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true; font-style: italic;">Reinforcement Learning: An Introduction</span><span style="border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true;">. MIT Press, Cambridge, MA, 1998. [</span><a style="border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true; font-weight: bold; color: rgb(2, 179, 228);" href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">web</a><span style="border: 0px; font-size: 1rem; line-height: 1.7; -en-paragraph: true;">]</span></div></div></div></div></div></div><ul style="padding: 0px 0px 0px 40px; border: 0px; font-variant-ligatures: normal; font-variant-caps: normal; font-size: 15px; line-height: 1.33333em; font-family: &quot;Open Sans&quot;, Helvetica, sans-serif; list-style: disc; color: rgb(79, 79, 79); letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In <span style="font-style: italic; border: 0px;">Proceedings of the Seventh International Conference on Machine Learning</span>, Austin, TX, 1990. [<a style="border: 0px; font-weight: bold; color: rgb(2, 179, 228);" href="https://webdocs.cs.ualberta.ca/~sutton/papers/sutton-90.pdf">pdf</a>]</li><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">Sutton and Barto. <span style="font-style: italic; border: 0px;">Reinforcement Learning: An Introduction</span>. MIT Press, Cambridge, MA, 1998. [<a style="border: 0px; font-weight: bold; color: rgb(2, 179, 228);" href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">web</a>]</li><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;"><a style="border: 0px; font-weight: bold; color: rgb(2, 179, 228);" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">RL course by David Silver</a> (videos, slides)</li><ul style=" padding: 0px 0px 0px 40px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: inherit; line-height: 1.33333em; font-family: inherit; vertical-align: baseline; list-style: disc;"><li style=" padding: 0px; border: 0px; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: 1rem; line-height: 1.7; font-family: inherit; vertical-align: baseline; list-style: disc;">Lecture 8: Integrating Learning and Planning [<a style="border: 0px; font-weight: bold; color: rgb(2, 179, 228);" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf">pdf</a>]</li></ul></ul><div><br/></div></body></html>